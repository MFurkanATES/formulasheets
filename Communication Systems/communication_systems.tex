

% Communication Systems D-ITET
% ===========================================================================
% @Author: Noah Huetter
% @Date:   2019-09-24 17:26:28
% @Last Modified by:   noah
% @Last Modified time: 2019-10-03 16:58:45
% ---------------------------------------------------------------------------

\documentclass[a4paper, fontsize=8pt, landscape, DIV=1]{scrartcl}
\usepackage{lastpage}
\usepackage{hyperref}
% Include general settings and customized commands
\input{settings/general}
\input{settings/commands}

% This package makes formulas a bit more compact but less beautiful
% \usepackage{newtxtext,newtxmath}


% \bibliography{semiconductordevices}
% \bibliographystyle{ieeetr}
\medmuskip=1mu

%change page style for header
\pagestyle{fancy}
\footskip 20pt

% Uncomment this line to make formulasheet ultra compact
% This removes
% - list of variables
% \newcommand{\makeultracompact}{irrelevant}
\let\makeultracompact\undefined

% Make stuff ultra compact if so desired
\ifdefined\makeultracompact
  \setlength{\parskip}{0pt}
  \setlength{\abovedisplayskip}{0pt}
  \setlength{\belowdisplayskip}{0pt}
  \setlength{\abovedisplayshortskip}{0pt}
  \setlength{\belowdisplayshortskip}{0pt}
\else
\fi
 
% -----------------------------------------------------------------------
\IfFileExists{../build/revision.tex}{
  \input{../build/revision.tex}
  \rhead{Compiled: \compiledate \hspace{1em} on: \hostname \hspace{1em} git-sha: \revision \hspace{1em} Noah Huetter}
}{\rhead{Noah Huetter}}

\ifdefined\makeultracompact
  \lhead{ETH Communication Systems 2019 \hspace{1em}compact version}
\else
  \lhead{ETH Communication Systems 2019}
\fi
\chead{\thepage}
\cfoot{}
\headheight 17pt \headsep 10pt
\title{ETH Communication Systems 2019}
\author{Noah Huetter}

\date{\today}
\begin{document}

\setcounter{page}{0}
\setcounter{secnumdepth}{2} %no enumeration of sections
\begin{multicols*}{4}
	\section*{Disclaimer}
	This summary is part of the lecture ``ETH Communication Systems'' (227-0121-00) by Prof. Dr. Armin Wittneben (FS19). It is based on the lecture. \\[6pt]
	Please report errors to \href{mailto:huettern@student.ethz.ch}{huettern@student.ethz.ch} such that others can benefit as well.\\[6pt]	
  The upstream repository can be found at \href{https://github.com/noah95/formulasheets}{https://github.com/noah95/formulasheets}
	\vfill\null
  \columnbreak
  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \tableofcontents
  \vfill\null
  %\columnbreak
  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\pagebreak
  \maketitle 
  \setcounter{page}{1}
  \thispagestyle{fancy}

  % ---------------------------------------------------------------------------
  \section{Random Processes}
  % ---------------------------------------------------------------------------
  % What is
  \cgraphic{0.8}{img/rp.png}
  A random process $X(t)$:
  \begin{itemize}
    \item is a sample space composed of (real valued) time functions: 
      $\{x_1(t), x_2(t), \dots, x_n(t)\}$
    \item observed at a fixed $t_k$ is a random variable 
      $X(t_k) = \{x_1(t_k), x_2(t_k), \dots, x_n(t_k)\}$
    \item The time function $x_s(t)$ is a \textbf{realization} (sample function)
    \item $x_s(t_k)$ observed at $t_k$ is a real number
    \item A stochastic process consists of infinitely many random variables, 
      one for each $t_k$, with the CDF $F_{\{X(t_k)\}}(x) = P(X(t_k)\leq x)$
  \end{itemize}


  \subsection{Stationary processes}
  A process is \textbf{Strict Sense Stationary (SSS)} if:
  \begin{itemize}
    \item $X(t)$ and $X(t+\tau)$ have same satistics $\forall \tau$
    \item The joint distribution function of a set of r.v. observed at times
      $t_1,\dots,t_n$ is invariant to a time-shift.
  \end{itemize}
  \begin{empheq}[box=\eqbox]{equation*}
    \begin{gathered}
      \forall n,\tau,t_1,\dots,t_n: \\
      F_{\{X(t_1+\tau),X(t_2+\tau),\dots,X(t_n+\tau)\}}(x_1,x_2,\dots,x_n) = \\
        F_{\{X(t_1),X(t_2),\dots,X(t_n)\}}(x_1,x_2,\dots,x_n)
    \end{gathered}
  \end{empheq}

  Properties:
  \begin{empheq}[box=\eqbox]{equation*}
    \begin{gathered}
      \forall t_k : \mu_X(t_k) = \mu_X \\
      \forall t_1, t_2 : R_X(t_1, t_2) = R_X(t_2-t_1) = R_X(\tau) \\
      C_X(t_1, t_2) = \E[(X(t_1)-\mu_X)(X(t_2)-\mu_X)] \\ = R_X(t_2-t_1) - \mu_X^2 \\
    \end{gathered}
  \end{empheq}
  
  A process is \textbf{Wide Sense Stationary (WSS)} if a r.p. has a \textit{constant} mean and the autocorrelation depends only on the \textit{time difference}.
  \begin{empheq}[box=\eqbox]{equation*}
    \begin{gathered}
      \forall t : \mu_X(t) = \mu_X \\
      \forall t_1, t_2 : R_X(t_1, t_2) = R_X(t_2-t_1) = R_X(\tau)
    \end{gathered}
  \end{empheq}

  Strict sense stationary $\implies$ wide sense stationary.


  \subsection{Mean and correlation}
  Defined as expectation of r.v. $X(t_k)$ by observing process at time $t_k$.
  \begin{empheq}[box=\eqbox]{equation*}
    \begin{gathered}
      \mu_X(t_k) = \E[X(t_k)] = \intinf x f_{\{X(t_k)\}}(x) \dx \\
    \end{gathered}
  \end{empheq}

  Autocorrelation function $R_X$ and autovariance function $C_X$ of a random process:
  \begin{empheq}[box=\eqbox]{equation*}
    \begin{gathered}
      R_{X}(t_{1},t_{2}) = \E[X(t_{1})X(t_{2})] \triangleq \\
      \intinf\intinf x_{1}x_{2}f_{X_{1},X_{2}}(x_{1}, x_{2})\dx_{1}\dx_{2}\\
      R_{XY}(x,y) = \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}xyf_{X,Y}(x, y)dxdy\\
      C_{X}(t_{1},t_{2}) = R_{X}(t_{2}-t_{1}) - m_{X}^{2}
    \end{gathered}
  \end{empheq}

  \begin{itemize}
    \item The mean and autocorrelation function determine the autocovariance function
    \item The mean and autocorrelation function only describe the first two moments of the process
  \end{itemize}

  Properties of the autocorrelation function:
  \begin{empheq}[box=\eqbox]{equation*}
    \begin{align*}
      \E[X^2(t)] &= R_X(0) & R_X(\tau) &= R_X(-\tau) \\
      \abs{R_X(\tau)} &\leq R_X(0)
    \end{align*}
  \end{empheq}
  \cgraphic{0.8}{img/autocorrelation.png}
  
  The Cross-correlation function $R_{XY}(t,u)$ of two random processes:
  \begin{empheq}[box=\eqbox]{equation*}
    \begin{gathered}
      R_{XY}(t,u) = \E[X(t)Y(u)] = \\
      \intinf xy\cdot f_{X,Y}(x, y) \dx \dy\\
    \end{gathered}
  \end{empheq}
  \begin{itemize}
    \item Stationariy menas $R_{XY}(t,u) = R_{XY}(\tau)$ for $\tau=t-u$
    \item Not generally an even function of $t$
    \item Not necessarily a maximum at $\tau=0$
    \item Symmetry: $R_{XY}(\tau) = R_{XY}(-\tau)$
  \end{itemize}

  \subsection{Ergodicity}
  Definition: A random process is \textit{ergodic} in the mean if
  \begin{itemize}
    \item Time average approaches ensemble averages for increasing $T$
    \item The variance of the time average approaches zero for incr. $T$
  \end{itemize}
  \begin{empheq}[box=\eqbox]{equation*}
    \begin{align*}
      \lim_{T\to\infty} \mu_X(T) &= \mu_X & \lim_{T\to\infty} \Var[\mu_X(T)] &= 0
    \end{align*}
  \end{empheq}

  Or in other words: The same behavior averaged over time as averaged over the space of all the system's states.

  \subsection{Filtered processes}
  Stationary random process $X(t)$ is input to a linear timeinvariant (LTI) filter with impulse response $h(t)$.
  \begin{empheq}[box=\eqbox]{equation*}
    \begin{gathered}
      Y(t) = \intinf h(\tau_1)X(t-\tau_1)\dtau_1 \\ S_Y(f) = \abs{H(f)}^2S_X(f)
    \end{gathered}
  \end{empheq}

  Find mean and autocorrelation of $Y(t)$:
  \begin{empheq}{gather*}
      \mu_X = \E[X(t)] \quad R_X(\tau) = \E[X(t)X(t-\tau)] \\
      \mu_Y = \E[Y(t)] = \E\left[\intinf h(\tau_1)X(t-\tau_1)\dtau_1\right]
  \end{empheq}

  Can interchange expectation and integration if stable $\intinf\abs{h(t)}\dt<\infty$ and finite mean $\mu_X<\infty$
  \begin{empheq}[box=\eqbox]{gather*}
      \mu_Y = \intinf h(\tau_1)\E[X(t-\tau_1)]\dtau_1 = \mu_X \intinf h(\tau_1) \dtau_1
  \end{empheq}

  Autocorrelation:
  \begin{empheq}{gather*}
      R_Y(t,u) = \E[Y(t)Y(u)] = \\ \E\left[\intinf h(\tau_1)X(t-\tau_1)\dtau_1
      \intinf h(\tau_2)X(u-\tau_2)\dtau_2\right] \\
  \end{empheq}

  Additional condition for interchange is finite mean-square value: $R_X(0) = \E[X^2(t)]<\infty$
  \begin{empheq}[box=\eqbox]{gather*}
      R_Y(\tau) = \intinf\intinf h(\tau_1)h(\tau_2)R_X(\tau-\tau_1+\tau_2)\dtau_1\dtau_2
  \end{empheq}

  (WS) stationary input process $X(t)$ to a stable LTI filter $\implies$ (WS) stationary output process $Y(t)$.

  \subsection{Power spectral density}
  \begin{empheq}[box=\eqbox]{equation*}
    S_{X}(f) = \mathscr{F}[R_{X}(\tau)](f) = \intinf R_{X}(\tau)e^{-j2\pi f\tau}\dtau 
        \end{empheq}
  \setlength{\leftmargini}{0.5cm} 
  \begin{itemize}
    \item $S_{X}(0) = \int_{-\infty}^{\infty} R_{X}(\tau)d\tau $
    \item $E[X^{2}(t)] = R_{X}(0) =  \int_{-\infty}^{\infty} S_{X}(f)df $
    \item $S_{X}(f) \ge 0$ $\forall f$
    \item $S_{X}(f)  = S_{X}(-f)$ $\forall f$, iff $X(t) \in \mathbb{R}$
  \end{itemize}
  \setlength{\leftmargini}{0pt} 

  \subsection{Gaussian process}
  Consider the r.v. $Y=\int_0^Tg(t)X(t)\dt$ where $g(t)$ is in an arbitraty function. If $Y$ is gaussian distributed, then the process $X(t)$  is a \textit{Gaussian process}
  \begin{itemize}
    \item A filtered Gaussian process remains a Gaussian process
    \item If $X(t)$ is a GP, the arbitrary set of r.v. $\vec{X} = [X(t_1),\dots,
    X(t_n)]^T$ is jointly gaussian distributed for any $n$
    \item The joint cdf is of these r.v. is completely determined by the \textbf{means} $\mu_X(t_i) = \E[X(t_i)]$ and \textbf{covariances} $C_X(t_k,t_i)=\E[(X(t_k)-\mu_X(t_k))(X(t_i)-\mu_X(t_i))]$
  \end{itemize}

  Multivariative Guass distribution:
  \begin{empheq}[box=\eqbox]{equation*}
    \begin{gathered}
      f(x) = \frac{\exp\left(-\frac{1}{2} (\vec{x}-\vec{m}_{x})^{T}\underline{\Sigma}^{-1} (\vec{x}-\vec{m}_{x}) \right)}{(2\pi)^{\frac{n}{2}}\det(\underline{\Sigma})^{\frac{1}{2}}}\\
      \underline{\Sigma} := 
      \begin{bmatrix} 
        \Cov(X_{1},X_{1})&...&\Cov(X_{1},X_{n})\\
        \vdotswithin{\ldots} & \vdotswithin{\ldots} & \vdotswithin{\ldots}\\
        \Cov(X_{n},X_{1})&...&\Cov(X_{n},X_{n})
      \end{bmatrix}
          \end{gathered}
  \end{empheq}

  \subsection{Noise}
  White noise is defined by its autocorrelation.
  \begin{empheq}[box=\eqbox]{align*}
      R_W(\tau) &= \frac{N_0}{2}\delta(t) & S_W(f) =  \frac{N_0}{2}
  \end{empheq}

  % ---------------------------------------------------------------------------
  % Week 2
  \section{Baseband Pulse Transmission}
  % ---------------------------------------------------------------------------
  Digital Baseband Pulse Transmission System: Based on the sample $y(t_i)$ 
  the receiver generates an estimate $\hat{a}_i$ of the amplitude $a_i$ of the
  transmitted pulse $g(t-iT_b)$.
  \cgraphic{1}{img/transmissionsystem.png}
  
  \subsection{Matched Filter}
  \cgraphic{0.9}{img/matchedfilter.png}
  \begin{empheq}{gather*}
      y(t) = g_0(t) + n(t) = h(t)*g(t) + h(t)*w(t)
  \end{empheq}

  Maximize \textit{pulse signal-to-noise ratio} $\eta$ at sampling time $t=T$:
  \begin{empheq}{gather*}
      \eta = \frac{\abs{g_0(T)}^2}{\E[n^2(t)]} = 
        \frac{\abs{\intinf H(f)G(f)e^{j2\pi fT}\df}^2}{\frac{N_0}{2}\intinf\abs{H(f)}^2\df}
  \end{empheq}

  Using Schwarz's inequality:
  \begin{empheq}[box=\eqbox]{gather*}
      \eta \leq \frac{2}{N_0}\intinf\abs{G(f)}^2\df
  \end{empheq}

  The quality sign (optimum) holds if $a(x)\propto b^*(x)$, i.e.
  \begin{empheq}[box=\eqbox]{gather*}
      H_{\text{opt}}(f) = k G^*(f)e^{-j2\pi fT} \Rightarrow h_{\text{opt}}(t) = k g(T-t)
  \end{empheq}

  The impulse response of the optimum filter, except for the scaling factor $k$, 
  is a time-reversed and delayed version of the input signal $g(t)$.

  The pulse SNR of a machted filter depends only on the ratio of the signal 
  energy $E$ to the PSD of the white noise at the input filter.
  \begin{empheq}{gather*}
      \eta_\text{max} = \frac{2}{N_0}\intinf \abs{G(f)}^2 \df = \frac{2E}{N_0} \\
      E = \intinf \abs{g(t)}^2\dt = \intinf \abs{G(f)}^2\df
  \end{empheq}

  \subsection{Error Rate}
  Discussed for a binary bipolar non-return-to-zero (NRZ) signal with amplitude $A$,
  bit duration $T_b$.
  \begin{empheq}{align*}
      x(t) &= \begin{cases}
              +A + w(t),& \text{Symbol 1 transmitted} \\
              -A + w(t),& \text{Symbol 0 transmitted}
              \end{cases} \\
      p_{\text{10}} &= \frac{1}{2}\erfc\left(\frac{A+\lambda}{\sqrt{N_0/T_b}}\right) =
       \Q\left(\sqrt{2}\frac{A+\lambda}{\sqrt{N_0/T_b}}\right) \\
      % {} &= \text{P}(y>\lambda | \text{symbol 0 was sent})
      {} &= \P(y>\lambda | \text{symbol 0 was sent})
  \end{empheq}

  The avg. prob. of symbol error $P_e$:
  \begin{empheq}[box=\eqbox]{gather*}
      P_e = \frac{p_0}{2} \erfc\left(\frac{A+\lambda}{\sqrt{N_0/T_b}}\right) + 
        \frac{p_1}{2} \erfc\left(\frac{A-\lambda}{\sqrt{N_0/T_b}}\right)
  \end{empheq}

  The error function:
  \begin{empheq}{align*}
      \P(n>a) \equiv \Q\left(\frac{a}{\sigma_n}\right) = \frac{1}{2}\erfc\left(\frac{1}{\sqrt{2}}\frac{a}{\sigma_n}\right)
  \end{empheq}

  Optimum decision threshold $\lambda$ that maximizes $P_e$:
  \begin{empheq}[box=\eqbox]{gather*}
      \lambda_\text{opt} = \frac{N_0}{4AT_b}\log\left(\frac{p_0}{p_1}\right)
  \end{empheq}

  \subsection{Intersymbol Interference}
  % \cgraphic{0.9}{img/isi.png}
  Arises when the channel is \textit{dispersive}, the magn. freq. resp. is not constant over the
  range of interest.
  \begin{empheq}{align*}
      s(t) &= \sum_k a_k \cdot g(t-kT_b) \\
      y(t) &= \mu\sum_k a_k\cdot p(t-kT_b) + n(t) \\
      t(t_i) &= \underbrace{\mu a_i}_\text{$i$-th bit} + \underbrace{\sum_{\substack{k=-\infty\\ k\neq i}}^{\infty} a_kp(i-k)T_b}_\text{ISI} + n(t_i)
  \end{empheq}

  \subsection{Nyquist's Criterion}
  In order to avoid ISI, we require $p(mT_b)=0$ for $m\neq0$ and obtain
  \begin{empheq}{gather*}
      \sum_{m=-\infty}^\infty p(mT_b)\delta(t-mT_b) = \delta(t) \laplace P_\delta(f)=1
  \end{empheq}

  An the nyquist criterion ($R_b = 1/T_b$ symbol rate):
  \begin{empheq}[box=\eqbox]{gather*}
      \sum_{n=-\infty}^\infty P(f-nR_b) = T_b
  \end{empheq}

  In words: The pulse function $P$ in freq. domain copied with spacing $R_b$ must be constant.

  \textbf{Ideal nyquist channel:} The simplest function $P(f)$ that satisfies this is the rectangular function (ideal LPF) with $W=R_b/2$, $R_b$ the nyquist rate.
  \begin{empheq}{align*}
      P(f) &= \frac{1}{2W}\rect\left(\frac{f}{2W}\right) = \begin{cases}
        \frac{1}{2W},&-W\leq f\leq W \\
        0,& \abs{f} > W
      \end{cases} \\
      p(t) &= \sinc(2Wt) \quad W = \frac{1}{2T_b} \quad E_b = \frac{A^2}{R_b}
  \end{empheq}

  \textbf{Raised Cosine Spectrum:} consists of flat portion and sinusodial rolloff.
  \cgraphic{0.8}{img/raisedcosine.png}
  \begin{empheq}{align*}
      P(f) &= \begin{cases}
        \frac{1}{2W} &  0\leq \abs{f} \leq f_1 \\
        \frac{1}{4W}\left(1-\sin\left[\frac{\pi(\abs{f}-W)}{2W-2f_1}\right]\right) 
          & \abs{f}\in[f_1, 2W-f_1]\\
        0 & \abs{f} > 2W-f_1
      \end{cases} \\
      p(t) &= \sinc(2Wt)\left(\frac{\cos(2\pi\alpha Wt)}{1-16\alpha^2W^2t^2}\right)\\
      \alpha &= 1-\frac{f_1}{W}  \in [0,1] \quad \text{Rollof factor}
  \end{empheq}

  Bandwidth is larger: $B_T = 2W-f_1 = W(1+\alpha$).

  \subsection{Correlative-Level Coding}
  Use basepulses which introduce controlled ISI. Same BW but higher $P_e$
  \cgraphic{1.0}{img/clc.png}
  \begin{empheq}{align*}
      H_I(f) &= \begin{cases}
        2T_b\cos(\pi fT_b)e^{-i\pi fT_b}, & \abs{f}<1/2T_b \\
        0, \text{else}
      \end{cases} \\
      h_I(t) &= \frac{T_b^2\sin(\pi t/T_n)}{\pi t(T_b-t)}
  \end{empheq}

  Decoding
  \cgraphic{0.6}{img/clc_dec.png}

  \textbf{Precoding} The decision feedback receiver is prone to propagating error. Using modulo-2 
  precoding, this can be omitted.
  \cgraphic{1}{img/precoding_oneline.png}
  \begin{empheq}[box=\eqbox]{gather*}
      d_k = b_k \oplus d_{k-1} \Rightarrow b_k = d_k \oplus d_{k-1} \\
      c_k = \begin{cases}
        0,&  b_k = 1\\
        \pm2,&  b_k = 0\\
      \end{cases}
  \end{empheq}

  \subsection{Baseband M-ary PAM Transmission}
  In a M-ary PAM system: M possible amplitude levels. One symbol encodes $\log_2M$ bits. 
  Thus the signal rate $T$ is related to the bit duration $T_b$ of a binary PAM as:
  \begin{empheq}{gather*}
      T = T_b\log_2M
  \end{empheq}
  \begin{itemize}
    \item For same avg. $P_e$, an M-ary PAM requires more Tx power
    \item If $M \gg 2$ the Tx energy per bit must be increased by $M^2/(3\log_2M)$ 
      for same $P_e$
  \end{itemize}


  % ---------------------------------------------------------------------------
  % Week 3
  \section{Signal Space Analysis}
  % ---------------------------------------------------------------------------
  Continuous AWGN (Additive white gaussian noise) channel.
  \begin{itemize}
    \item All symbols $m_i$ from source are eually likely $p_i=p(m_i)=\frac{1}{M}$
    \item Transmitter codes each $m_i$ into a signal $s_i(t)\in \{s_k(t) | 1\leq k \leq M\}$
    \item Cahnnel adds AWGN $x(t) = s_i(t) + w(t)$ for $0\leq t\leq T$
    \item The optimal receiver minimizes the avg. pob. of symbol error $P_e$
  \end{itemize}
  \begin{empheq}{gather*}
    P_e = \sum_{i=1}^M p_i \P(\hat{m}\neq m_i | m_i)
  \end{empheq}


  \subsection{Geometric Signal Representation}
  Let $\{\phi_i(t)\}_{i=1\dots N}$ be a set of othonormal basis fuctions of the signal set
  $\{s_i(t)\}_{i=1\dots M}$. All signals can be expressed as a finite sum.
  The coeff. $s_{ij}$ are given by the projection onto $\{\phi_i(t)\}_{i=1\dots N}$.

  The orthonormal functions deine a $N$-dimensional Eucledian space - the signal space.
  \begin{empheq}[box=\eqbox]{gather*}
      \int\limits_0^T \phi_i(t)\phi_j(t)\dt = \delta_{ij} = \begin{cases}
        1,&  i=j\\
        0,&  -\neq j\\
      \end{cases} \\
      s_i(t) = \sum_{j=1}^N s_{ij}\phi_j(t)  \quad
      s_{ij} = \int_0^T s_i(t)\phi_j(t)\dt\\
      0\leq t \leq T,\quad i=1\dots M,\quad j=1\dots N
  \end{empheq}

  \cgraphic{0.8}{img/sssynth.png}

  \begin{empheq}{gather*}
      \langle s_i(t), s_k(t) \rangle = \int_0^T s_i(t)s_k(d)\dt = \vect{s}_i^\top \cdot \vect{s}_k \\
      \norm{\vect{s}_i}^2 = \langle s_i(t), s_i(t) \rangle = \int_0^T s_i(t)^2 \\
      \norm{\vect{s}_i-\vect{s}_k}^2 = \sum_{j=1}^N(s_{ij}-s_{kj})^2 = \int_0^T (s_i(t)-s_k(t))^2 \dt \\
      \cos\theta_{jk} = \frac{\vect{s}_i^\top \cdot\vect{s}_k}{\norm{\vect{s}_i}\cdot\norm{\vect{s}_k}} \quad
      E_i = \sum_{j=1}^Ns_{ij}^2 = \norm{\vect{s}_i}^2
  \end{empheq}

  \textbf{Gram-Schmidt orthogonalization procedure}: Start with a complete system $s_1(t),\dots,s_M(t)$
  that generates the signal space. At each step generate a new basis function $\phi_i$.
  The basis has only $N\leq M$ functions.

  \begin{enumerate}
    \item Build basis function $\phi_1$ from $s_1$
    \begin{align*}  
      \phi_1(t) = \frac{s_1(t)}{\sqrt{\int_0^Ts_1^2(t)\dt}}
    \end{align*} 
    
    \item Search for a basis function from $s_2(t)$
    \begin{align*}  
      s_{21} &= \langle s_2(t), \phi_1(t) \rangle = \int_0^Ts_2(t)\phi_1(t)\dt \\
      g_2(t) &= s_2(t) - s_{21}\phi_1(t)
    \end{align*} 
    If $g_2=0$, $s_2$ is lin. dep. on $\phi_1$ and does not lead to a new basis function.
    Otherwise:
    \begin{align*}  
      \phi_2(t) = \frac{g_2(t)}{\sqrt{\int_0^Tg_2^2(t)\dt}}
    \end{align*} 
    
    \item Search for a basis function from $s_3(t)$
    \begin{align*}  
      s_{31} &= \langle s_3(t), \phi_1(t) \rangle \quad
      s_{32} = \langle s_3(t), \phi_2(t) \rangle\\
      g_3(t) &= s_3(t) - s_{31}\phi_1(t) - s_{32}\phi_2(t)
    \end{align*} 
    If $g_3=0$, $s_3$ is lin. dep. on $\phi_1$ and $\phi_2$ and does not lead to a new basis function.
    Otherwise:
    \begin{align*}  
      \phi_3(t) = \frac{g_3(t)}{\sqrt{\int_0^Tg_3^2(t)\dt}}
    \end{align*} 

    \item Search for a basis function from $s_M(t)$. Project $s_M$ on the already determined
    basis functions, decompose $S_M$ into its projection and a difference term $g_M$. 
    If $g_M\neq 0$:
    \begin{align*}  
      \phi_N(t) = \frac{g_M(t)}{\sqrt{\int_0^Tg_M^2(t)\dt}}
    \end{align*} 

  \end{enumerate}

  \subsection{Discrete System Model}
  The signal vector $\vect{s}$, noise vector $\vect{w}$ and the received signal $\vect{x}$.
  \begin{empheq}{gather*}
      \vect{s}_i = \begin{bmatrix} s_{i1} & \hdots & s_{iN} \end{bmatrix}^\top \quad
      \vect{w}   = \begin{bmatrix} w_{1} & \hdots & w_{N} \end{bmatrix}^\top \\
      \vect{x}   = \begin{bmatrix} x_{1} & \hdots & x_{N} \end{bmatrix}^\top = \vect{s}_i + \vect{w} \\
      \E[w_j] = 0 \quad \E[w_j\cdot w_k] = \delta_{jk} \quad \Var(w_j) = \frac{N_0}{2}
  \end{empheq}

  % \cgraphic{0.4}{img/awgnvectorchannel.png}
  \textbf{Theorem of Irrelevance} For signal detection with AWGN, only the projection of the noise
  onto the basis functions of the signal set $\{s_i(t)\}_{i=1}^M$ affect the sufficient statistics
  of the detection problem. The remainder of the noise is irrelevant.
  \begin{empheq}[box=\eqbox]{gather*}
      \mu_{X_j} = \E[X_j] = \E[s_{ij} + W_j] = s_{ij} + \E[W_j] = s_{ij} \\
      \sigma_{X_j}^2 = \Var(X_j) = \E[(X_j-s_{ij)^2}] = \E[W_j^2] = \frac{N_0}{2} \\
      W_j = \int_0^TW(t)\phi_j(t)\dt
  \end{empheq}

  The elements $X_j$ and $X_k$ of the received signal vector have the covariance
  \begin{empheq}{gather*}
      \Cov(x_j,x_k) = \E[(x_j-\mu_{x_j})(x_k-\mu_{x_k})] = 0, \quad j\neq k
  \end{empheq}
  Thus the $x_j$ are mutually uncorrelated. $\implies$ statistical independence.

  \textbf{Likelihood Function} As the $x_j$ are statistically indep. the conditional PDF of $\vect{x}$
  given $\vect{s}$ (i.e. symbol $m_i$ sent usign signal $s_i$) follows:
  \begin{empheq}[box=\eqbox]{gather*}
      L(\vect{s}_i) \coloneqq f_x(\vect{x}|\vect{s}_i) = f_W(\vect{w} = \vect{x}-\vect{s}_i) = 
      %= \prod_{j=1}^Nf_W(w_j=x_j-s_{ij})
      \\
       = \frac{1}{(\pi N_0)^{N/2}}\exp\left[-\frac{1}{N_0}\sum_{j=1}^N(x_j-s_{ij})^2\right] \\
       l(\vect{s}_i) = \log L(\vect{s}_i) = -\frac{1}{N_0}\sum_{j=1}^N(x_j-s_{ij})^2 + c
  \end{empheq}
  \begin{empheq}{gather*}
      c = -\frac{N}{2}\log(\pi N_0) \quad i\in \{1,\dots,M\}
  \end{empheq}

  $L$ likelihood function, $l$ log-likelihood function can be used because the pdf is always nonnegative
  and monot. incr.
  The constant $c$ is indep. of hyp. $\vect{s}_i$ and can be discarded for the decision.

  \subsection{Detection and Decoding}
  Detection problem: Given the observation $\vect{x}$, determine an estimate $\hat{m}$ of the transmitted
  symbol $m_i$, s.t. the probability of error is minimized.
  \cgraphic{0.5}{img/nosiecloud.png}

  \begin{empheq}[box=\eqbox]{gather*}
      P_e(m_i | \vect{x}) = \P(m_i \text{not sent} | \vect{x}) = 1-\P(m_i\text{sent} | \vect{x}) \\
  \end{empheq}
  
  The MAP (Maximum-A-Posteriori) decision rule is optimum in the minimum prob. of error sense. Set $\hat{m}=m_i$ if:
  \begin{empheq}[box=\eqbox]{gather*}
    \P(m_i\text{sent}|\vect{x}) \geq \P(m_k\text{sent}|\vect{x}) \quad \forall k\neq i
  \end{empheq}

  Rephrased using Baye's rule, set $\hat{m}=m_i$ if ($p_k$ a priori prob. of transmitting $m_k$, 
  $f_x(\vect{x}|m_k)$ cond. pdf of $\vect{x}$ given $m_k$):
  \begin{empheq}[box=\eqbox]{gather*}
    \hat{m} = \argmax_{m_k} \frac{p_k\cdot f_x(\vect{x}|m_k)}{f_x(\vect{x})} \quad \forall k\neq i
  \end{empheq}

  We can drop $f_x(\vect{x})$ as it is indep. of the symbol decision. For equiprobable source symbols,
  we obtain the ML decision rule: Set $\hat{m}=m_i$ if $l(m_k)$ max. for $k=i$.

  \textbf{Simplfied ML Rule}: $\vect{x}$ lies in region $Z_i$ if
  \begin{empheq}{gather*}
      \sum_{j=1}^Nx_js_{kj}-\frac{1}{2}E_k
  \end{empheq}
  is maximum for $k=i$.

  \textbf{Correlation receiver}
  \cgraphic{1.0}{img/crrelationrx.png}

  \subsection{Probability of Error}
  $\P(A_{ik})=P_2(\vect{s}_i,\vect{s}_k)$ is the pairwise error prob. that observation $\vect{x}$ is closer
  to $\vect{s}_k$ than to $\vect{s}_i$:
  \begin{empheq}{gather*}
    P_2(\vect{s}_i,\vect{s}_k) = \P(\norm{\vect{x}-\vect{s}_k}^2 < \norm{\vect{x}-\vect{s}_i}^2)
  \end{empheq}
  
  With the eucledian distance $d_{14}\coloneqq\norm{\vect{s}_1-\vect{s}_4}$:
  \begin{empheq}[box=\eqbox]{gather*}
    P_2(\vect{s}_1,\vect{s}_2) = \P(z<\frac{1}{2}d_{14}) = Q\left(\frac{d_{14}}{\sqrt{2M_0}}\right) \\
    = \frac{1}{2}\erfc\left(\frac{d_{14}}{2\sqrt{N_0}}\right)
  \end{empheq}

  \textit{The pairwise probability of error only depends on the Euclidean distance and is e.g. invariant 
  to rotation and translation of the signal constellation}

  From the union bound we have
  \begin{empheq}{gather*}
    P_e(m_i) \leq \sum_{\substack{k=1\\k\neq i}}^MP_2(\vect{s}_i,\vect{s}_k)
  \end{empheq}

  $P_e$ is the error prob. averaged over all symbols. An upper bound follows as
  \begin{empheq}{gather*}
    P_e = \sum_{i=1}^Mp_iP_e(m_i)\leq\frac{1}{2}\sum_{i=1}^M\sum_{\substack{k=1\\k\neq i}}^Mp_i\erfc\left(\frac{d_{ik}}{2\sqrt{N_0}}\right)
  \end{empheq}


  % ---------------------------------------------------------------------------
  % Basics
  \vfill\null
  \pagebreak
  \section{Math}
  % ---------------------------------------------------------------------------
  \subsection{General}
  \begin{empheq}{align*}
      &\cos(a)\cos(b) + \sin(a)\sin(b) = \cos(a-b) \\
      &\sinc(x) = \frac{\sin(\pi x)}{\pi x} \\
      &\sin(x) = \frac{e^{ix}-e^{-ix}}{2i} \quad \cos(x) = \frac{e^{ix}+e^{-ix}}{2}
  \end{empheq}

  \subsection{Fourier Transform}
  \begin{empheq}{align*}
    \rect\left(\frac{t}{T}\right) &\laplace T\sinc(fT) \\
    \sinc\left(\frac{t}{T}\right) &\laplace T\rect(fT) \\
  \end{empheq}



\end{multicols*}

\setcounter{secnumdepth}{2}
\end{document}
