

% ETH Systems-on-chip for Data Analytics and Machine Learning 2020
% ===========================================================================
% @Author: Noah Huetter
% @Date:   2020-02-18 17:26:28
% @Last Modified by:   noah
% @Last Modified time: 2020-03-05 16:49:46
% ---------------------------------------------------------------------------

\documentclass[a4paper, fontsize=8pt, landscape, DIV=1]{scrartcl}
\usepackage{lastpage}
\usepackage{hyperref}
\usepackage{dirtytalk}
% Include general settings and customized commands
\input{settings/general}
\input{settings/commands}

% This package makes formulas a bit more compact but less beautiful
% \usepackage{newtxtext,newtxmath}

% scala language description
\lstdefinelanguage{BNF}{%
    alsoletter={-},%
    sensitive,%
}[keywords,comments]%

\lstset{%
    basicstyle=\ttfamily,%
%    language=P4,%
    aboveskip=3mm,%
    belowskip=3mm,%
    fontadjust=true,%
%    columns=[c]fixed,%
    keepspaces=true,%
%    commentstyle=\itshape,%
    frame=single,
    keywordstyle=\bfseries,%
    captionpos=b,%
    framerule=0.3pt,%
    firstnumber=0,%
    numbersep=1.5mm,%
    numberstyle=\tiny,%
    breaklines=true,
    postbreak=\mbox{\textcolor{red}{$\hookrightarrow$}\space},
}


\lstnewenvironment{algorithm}[1][] %defines the algorithm listing environment
{   
  \lstset{ %this is the stype
    mathescape=true,
    frame=tB,
    numbers=none, 
    numberstyle=\tiny,
    basicstyle=\scriptsize, 
    keywordstyle=\color{black}\bfseries\em,
    keywords={,input, output, return, datatype, function, in, if, else, foreach, while, begin, end, } %add the keywords you want, or load a language as Rubens explains in his comment above.
    numbers=left,
    xleftmargin=.0\textwidth,
    #1 % this is to add specific settings to an usage of this environment (for instnce, the caption and referable label)
  }
}
{}

% \bibliography{semiconductordevices}
% \bibliographystyle{ieeetr}
\medmuskip=1mu

%change page style for header
\pagestyle{fancy}
\footskip 20pt

% Uncomment this line to make formulasheet ultra compact
% This removes
% - list of variables
% \newcommand{\makeultracompact}{irrelevant}
\let\makeultracompact\undefined

% Make stuff ultra compact if so desired
\ifdefined\makeultracompact
  \setlength{\parskip}{0pt}
  \setlength{\abovedisplayskip}{0pt}
  \setlength{\belowdisplayskip}{0pt}
  \setlength{\abovedisplayshortskip}{0pt}
  \setlength{\belowdisplayshortskip}{0pt}
\else
\fi
 

% Outlines configurations
\makeatletter
% the outline environment provides commands \1..\4 for
% introducing items at level 1..4, and \0 for normal paragraphs
% within the outline section.
\renewenvironment{outline}[1][]{%
  \ifthenelse{\equal{#1}{}}{}{\renewcommand{\ol@type}{#1}}%
  \ol@z%
  \newcommand{\0}{\ol@toz\ol@z}%
  \newcommand{\1}{\vspace{\dimexpr\outlinespacingscalar\baselineskip-\baselineskip}\ol@toi\ol@i\item}%
  \newcommand{\2}{\vspace{\dimexpr\outlinespacingscalartwo\baselineskip-\baselineskip}\ol@toii\ol@ii\item}%
  \newcommand{\3}{\vspace{\dimexpr\outlinespacingscalar\baselineskip-\baselineskip}\ol@toiii\ol@iii\item}%
  \newcommand{\4}{\vspace{\dimexpr\outlinespacingscalar\baselineskip-\baselineskip}\ol@toiiii\ol@iiii\item}%
}{%
  \ol@toz\ol@exit%
}
\makeatother
\def\outlinespacingscalar{0.5}
\def\outlinespacingscalartwo{0.5}

% -----------------------------------------------------------------------
\IfFileExists{../build/revision.tex}{
  \input{../build/revision.tex}
  \rhead{Compiled: \compiledate \hspace{1em} on: \hostname \hspace{1em} git-sha: \revision \hspace{1em} Noah Huetter}
}{\rhead{Noah Huetter}}

\ifdefined\makeultracompact
  \lhead{ETH Systems-on-chip for Data Analytics and Machine Learning 2020 \hspace{1em}compact version}
\else
  \lhead{ETH Systems-on-chip for Data Analytics and Machine Learning 2020}
\fi
\chead{\thepage}
\cfoot{}
\headheight 17pt \headsep 10pt
\title{ETH Systems-on-chip for Data Analytics and Machine Learning 2020}
\author{Noah Huetter}

\date{\today}
\begin{document}

\setcounter{page}{0}
\setcounter{secnumdepth}{2} %no enumeration of sections
\begin{multicols*}{4}
	\section*{Disclaimer}
	This summary is part of the lecture ``Systems-on-chip for Data Analytics and Machine Learning'' (227-0150-00L) by Prof. Dr. Luca Benini (FS20). \\[6pt]
	Please report errors to \href{mailto:huettern@student.ethz.ch}{huettern@student.ethz.ch} such that others can benefit as well.\\[6pt]	
  The upstream repository can be found at \href{https://github.com/noah95/formulasheets}{https://github.com/noah95/formulasheets}
	\vfill\null
  \columnbreak
  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \setcounter{tocdepth}{2}
  \tableofcontents
  \vfill\null
  %\columnbreak
  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\pagebreak
  \maketitle 
  \setcounter{page}{1}
  \thispagestyle{fancy}

  % ---------------------------------------------------------------------------
  \section{Architecture review, MCUs}
  % ---------------------------------------------------------------------------
  \subsection{Datacenters}
  \subsubsection{Cloud Service Models}
  \begin{outline}
    \1 Software as a Service (Saas)
      \2 Provider licenses applications to users as a service
      \2 Avoid costs of installation, maintenance, patches, ...
    \1 Platform as a Service (Paas)
      \2 Provider offers software platform for building applications
      \2 e.g. Google's App-Engine
      \2 Avoid worrying about scalability of platform
    \1 Infrastructure as a Service (Iaas)
      \2 Povider offers raw computing, storage and network
      \2 e.g. Amazon AWS
      \2 Avoid buying servers and estimating resource needs
  \end{outline}

  \subsubsection{Building blocks of moden data centers}
  \begin{outline}
    \1 Network Switch
    \1 Rack
    \1 Server Racks
    \1 Cluster Switch
  \end{outline}
  Rack of servers (so called commodity servers) consist of a modular design with
  top-of-rack switch, power, network and storage. The ToR is the link aggregate to 
  the next level.

  A data center is not just a collectino of servers, it's a "small internet". It is
  administered as a single domain that does not have to be compatible with the "outside
  world". No need for international standards.

  There exists specialized machine learning cloud hardware.

  \subsubsection{Performance Metrics}
  \begin{outline}
    \1 Throughput
      \2 Requests per second
      \2 Concurrent users
      \2 Gbytes/sec processed
    \1 Latency
      \2 Execution time
      \2 Per request latency
  \end{outline}

  \subsubsection{Tail Latency}
  Output depends on all servers finishing. Probability of a slow server $P_\text{slow}$.
  Job on $N$ servers finishes when the last server is done. Even if $P_\text{slow}$ is
  very small, $P_\text{jobFsat}$ is small if $N$ is large.
  \begin{empheq}[box=\eqbox]{equation*}
    \begin{gathered}
      P_\text{jobFast} = (1-P_\text{slow})^N
    \end{gathered}
  \end{empheq}
  \cgraphic{1.0}{img/tail.jpg}

  \subsubsection{TCO: Total Cost of Ownership}
  TCO = capital (CapEx) + operational (OpEx) expenses. CapEx: building, generators, HW.
  OpEx: Elec., repairs, insurance. Users perspective: CapEx: cost of long term leases on HW and
  services. OpEx: Pay per use cost on HW and services

  \subsubsection{Reliability}
  \begin{outline}
    \1 Failure in time (FIT)
      \2 Failures per billion hours of operation
    \1 Mean time to failure (MTTF)
      \2 Time to produce first incorrect output
    \1 Mean time to repair (MTTR)
      \2 Time to detect and repair a failure
  \end{outline}

  Steady state availability = MTTF/(MTTF + MTTR).

  \subsubsection{Multicore}
   \begin{tabularx}{\linewidth}{l c c c}
    \hline
    {} & Single & Dual & Quad \\ \hline
    Core area & $A$ & $\approx A/2$ & $\approx A/4$ \\
    Core power & $W$ & $\approx W/2$ & $\approx W/4$ \\
    Chip power & $W+O$ & $W+O'$ & $W+O''$ \\
    Core performance & $P$ & $0.9P$ & $0.8P$ \\
    Chip performance & $P$ & $1.8P$ & $3.2P$ \\ \hline
  \end{tabularx}

  \subsubsection{Amdahl's Law}
  Not all operations can run in parallel, hence speedup is limited. $f$ fraction 
  that can run in parallel.
  \begin{empheq}[box=\eqbox]{equation*}
    \begin{align}
      \text{Speedup} &= \frac{1}{(1-f)+\frac{f}{n}} & \lim_{n\to\infty}&\frac{1}{1-f+\frac{f}{n}}=\frac{1}{1-f}
    \end{align}
  \end{empheq}
  Amdahl's Law ignores power cost of $n$ cores. More cores resupts in each core being slower. 
  Parallel speedup $< n$. Seiral portion takes longer, also, inerconnect and scaling overhead.
  Fixed power budget forces slow cores, serial code quickly dominates.
  \cgraphic{1.0}{img/amdahl.png}

  \subsection{Computing Systems Performance}
  \subsubsection{Instruction Count and CPI}
  CPI: Cycles per instruction. CPU time = instruction count x CPI x clock cycle time.
  CPI is determined by program, ISA and compiler. Different instructions have diferent CPI.

  \subsubsection{Performance}
  IC: Instruction count.
  Depends on
  \begin{outline}
    \1 Algorithm: affects IC, possibly CPI
    \1 Programming language, affects IC, CPI
    \1 Compiler: affects IC, CPI
    \1 Instruction set architecture: affects IC, CPI, Tc
  \end{outline}

  \subsubsection{Power}
  \begin{empheq}[box=\eqbox]{equation*}
    \begin{gathered}
      \text{Power} = \text{Capactivice load} \times \text{Voltage}^2 \times \text{Frequency}
    \end{gathered}
  \end{empheq}

  \subsubsection{Multiprocessors}
  More than one processor per chip. Requires explicitly parallel programming.
  Hard to rogramm for performance, balance load and optimize synchronization.

  \subsection{Processor Architecture}
  CPU performance factors are determined by instruction count, CPI and cycle time.

  \subsubsection{Instruction Execution}
  Programm counter (PC) points to instruction memory to fetch instructions. Register
  numbers point to the register file to read registers. Depending on instruction class,
  use the ALU for arithmetic results, memory address calculation or branch compare. 
  Access data memory for load/store and increment PC by target address of 4 (4 byte instruction
  words).
  \cgraphic{1.0}{img/archoverview.png}

  \subsubsection{R-format instructions}
  Read two register from register file, perform arithmetic/logical operation and
  write back to register.

  \subsubsection{Load/Store Instructions}
  Read register operands, calculate address using offset, load memory to register
  or vice versa.

  \subsubsection{Branch Instructions}
  Read register, compare operands, calculate target address.

  \subsubsection{Pipelining}
  RISC-V is designed for pipelining: All instructions 32-bit, few regular instruction
  format, load/store addressing.

  \subsubsection{Hazards}
  Situations that precent starting the next instruction in the next cycle.
  \begin{outline}
    \1 Structure hazard
      \2 A required resource is busy
      \2 Fix: requires separate instruction/data memories
    \1 Data hazard
      \2 Need to wait for precious instruction to complete its data read/write
      \2 Fix: Forward data to next op without storing in register
      \2 Fix: Code scheduling to avoid stalls
    \1 Control hazard
      \2 Deciding on control action depends on precious instruction
      \2 Fix: Branch prediction
  \end{outline}

  \subsubsection{Branch Prediction}
  Predict outcome of branch and only stall if prediction is wrong. RISC-V can predict branches
  not taken. Static prediction based on typical branch behaviour. Dynamic measures actual branch
  behaviour, e.g. record recent history of each branch. Assume future behav. will continue the trend,
  when wrong, stall while re-fetching. 2-Bit predictor changes prediction only on two successive
  mispredictions.
  \cgraphic{1.0}{img/pipeline.png}

  \subsection{Exploiting Memory Hieararchy}
  Programs access a small portion of their address space at any time. Temporal locality aims at items
  that are accessed recently to be likely accessed again soon (e.g. loop). Spatial locality towards
  items near recently accessed (e.g. sequential instructions, array data).

  \subsubsection{Locality}
  Copy recently accessed (and nearby) items from disk to smaller DRAM (main memory) and from there 
  to smaller SRAM (cache attahched to CPU). If accessed data is present, \textbf{hit}, else \textbf{miss}.
  Hit ratio: hits/accesses.

  \subsubsection{Cache Memory}
  \begin{outline}
    \1 Direct Mapped Cache
      \2 Location determined by address
      \2 Direct mapped: only one choice
      \2 Store tags (high order bits of source data) and valid flag if there is data at that location
    \1 Larger block size
      \2 Store tag, index and offset
      \2 Larger blocks should reduce miss rate but larger miss penalty
    \1 Associative Cache
      \2 Allow a given block to go in any cache entry
      \2 Requires all entries to be searched at once
    \1 $n$-way set associative
      \2 Each set contains $n$ entries
      \2 Blcok number determines which set
      \2 Search all entries in a given set at once
  \end{outline}

  \subsubsection{AMAT: Average memory access time}
  AMAT = Hit time + miss rate x miss penalty

  \subsubsection{Replacement Policy}
  Direct mapped: no choice. Set associative: Prefer non-valid entry, if there is one. Otherwise,
  choose among entries in the set. Least recently used (LRU): Choose the one unused for the longest time.
  Random: Gives approx. same performance as LRU.

  % ---------------------------------------------------------------------------
  \section{(Deep) NN Workloads}
  % ---------------------------------------------------------------------------

  \subsection{Artificial Intelligence}
  John McCarthy: \say{The science and engineering of creating intelligen machines}
  \cgraphic{0.7}{img/aicloud.png}

  \subsection{Linear Classifier}
  If we want to classify an $32\times32$ image into $10$ classes we can use the 
  linear classifier:
  \begin{empheq}[box=\eqbox]{equation*}
    \begin{gathered}
      f(x,W) = Wx + b
    \end{gathered}
  \end{empheq}
   \begin{tabularx}{\linewidth}{c c l}
    \hline
    {} & Dimension & Description \\ \hline
    $W$ & $10 \times 3072$ & parameters or weights \\
    $b$ & $10 \times 1$ & bias or offset \\
    $x$ & $3072 \times 1$ & Input, e.g. 32x32x3 image pixels \\ \hline
  \end{tabularx}
  Every column in $W$ defines a plane. One side of this plane corresponds to images
  belonging to this class, the other side meaning it does not belong to this class.

  To evaluate a linear classifier, a loss function $L_i$ is defined. Given a dataset
  of examples $\{(x_i,y_i)\}_{i=1}^N$ the loss over the dataset is a sum of losses:
  \begin{empheq}[box=\eqbox]{equation*}
    \begin{gathered}
      L = \frac{1}{N}\sum_i L_i\left(f(x_i,W), y_i\right)
    \end{gathered}
  \end{empheq}
  $x_i$ image \\
  $y_i$ is (integer) label

  \subsubsection{Multiclass SVM loss}
  The SVM (support vector machine) loss function with the shorthand $s=f(x_i,W)$:
  \begin{empheq}[box=\eqbox]{equation*}
    \begin{align}
      L_i &= \sum_{j\neq y_i} \begin{cases}
          0 & s_{y_i}\geq s_j+1 \\ 
          s_j-s_{y_i}+1 & \text{otherwise}
        \end{cases} \\
      &=\sum_{j\neq y_i}\max(0, s_j-s_{y_i}+1)
    \end{align}
  \end{empheq}

  \cgraphic{0.5}{img/linlossex.png}

  \subsubsection{Other cost functions}
  \begin{empheq}[box=\eqbox]{equation*}
    \begin{align}
      \text{Softmax} &&& -\log\left(\frac{e^{sy_i}}{\sum_je^{sy_j}}\right) \\
      \text{SVM} &&& \sum_{j\neq y_i}\max(0, s_j-s_{y_i}+1) \\
      \text{Full loss} &&& \frac{1}{N}\sum_{i=1}^NL_i+R(W)
    \end{align}
  \end{empheq}

  \subsubsection{Regularization}
  Model should be "simple", so it works on test data.
  \begin{empheq}[box=\eqbox]{equation*}
    \begin{gathered}
      L(W) = L + \lambda R(W)
    \end{gathered}
  \end{empheq}

  \subsection{Training Linear Classifier}
  How do we find the best $W$ using a dataset, a score function ($s=f(x,W)$) and a
  loss function? Enter \textbf{gradiend-based optimization}: The slope in any direction
  is the dot product of the direction with the gradient.

  \begin{lstlisting}[language=Python]
while True:
  weights_grad = eval_gradient(loss_fun, data, weights)
  weights += -step_size*weights_grad\end{lstlisting}

  \subsubsection{Stochastic Gradient Descent (SGD)}
  The full sum in the cost function is expensive. Aprroximate it using a \textbf{minibatch}
  of randomly selected data (32/64/128 common).
  \begin{lstlisting}[language=Python]
while True:
  data_batch = sample_train_dat(data, 64)
  weights_grad = eval_gradient(loss_fun, data_batch, weights)
  weights += -step_size*weights_grad\end{lstlisting}

  \subsubsection{Hard cases}
  Most data are not linearly seperable.
  \cgraphic{1.0}{img/linclassfail}

  \subsection{From linear to non-linear classifiers}
  Instead of feeding the classifier with raw data, perform a non-linear transformation
  beforehand so that the data becoms linearly seperable. This is called the \textbf{Kernel Trick}.
  
  Later we will see that convolutional netowrks can work on raw data so that no
  effort has to be made on pre-processing.

  \subsection{Neural Networks}
  Consists of interconnected cells. Each cells has multiple inputs that are weighted
  and added (like a linear classifier), but then put through a non-linear activation
  function.
  \cgraphic{0.7}{img/neuralcell.png}

  Multiple cells are connected in several layers.
  \cgraphic{0.7}{img/layers.png}

  \subsection{Deep Learning}
  A deep neural network is a neural network with a lot of hidden layers.

  \subsection{Training Neural Networks}
  Neural nets are trained based on gradient descent using backpropagation. To compute the
  gradient, input values are propagated from front to back through the network and then
  the partial derivatives calculated during backpropagation.
  \cgraphic{0.7}{img/backprop.png}

   \begin{tabularx}{\linewidth}{l l}
    \hline
    \textbf{add} gate & gradient distributor \\
    \textbf{max} gate & gradient router \\
    \textbf{mul} gate & gradient switcher \\ \hline
  \end{tabularx}

  \subsubsection{Mini-batch SGD}
  \begin{enumerate}
    \item \textbf{Sample} a batch of data
    \item \textbf{Forward} propagate it through the graph, get loss
    \item \textbf{Backward} propagate to calculate the gradients
    \item \textbf{Update} the parameters using the gradient
  \end{enumerate}

  \subsubsection{Non-linearity}
  The simplest is the so called \textbf{ReLU} (rectified linear unit).
  \begin{empheq}[box=\eqbox]{equation*}
    \begin{gathered}
      f(x) = \max(0,x)
    \end{gathered}
  \end{empheq}
  \begin{outline}
    \1 Does not saturate
    \1 Very computationally efficient
    \1 Converges much faster than sigmoid/tanh in practice
    \1 Actually more biologically plausible than sigmoid
  \end{outline}


  \subsection{Convolutional Neural Networks}
  Convolutional Neural Networks are neural networks with feed forward and sparsely-connected
  with weight sharing. They are trainedusing supervised learning (training set has inputs and
  outputs, i.e., labeled). The main two layers are \textbf{convolutional layers}
  and \textbf{fully connected} layers.
  \cgraphic{1.0}{img/cnntop.png}

  The convolution layer can further be dissected into convolution, non-linearity,
  norm and pooling.

  \subsubsection{Convolution}
  The input fmap (feature map) is element-wise multiplied with filter coefficient and
  partially summed to get one entry of the output fmap. The filter is then shifted
  one entry to get the next output value and so on.
  \cgraphic{0.85}{img/conv.png}

  There can be many input channels $C$, filters $M$ and output channels $M$.
  \cgraphic{0.85}{img/batching.png}
  \begin{empheq}[box=\eqbox]{equation*}
    \begin{align}
      &O[n][m][x][y] = \text{Activation}(\vect{B}[m]) + \\
      &\sum_{i=0}^{R-1}\sum_{j=0}^{S-1}\sum_{k=0}^{C-1}\vect{I}[n][k][Ux+i][Uy+j]\times\vect{W}[m][k][i][j]
    \end{align}
  \end{empheq}
  \begin{empheq}[]{equation*}
    \begin{gathered}
      0\leq N, -\leq m < M, 0 \leq y < E, 0\leq x < F\\
      E=(H-R+U)/U, F=(W-S+U)/U
    \end{gathered}
  \end{empheq}
  \begin{tabularx}{\linewidth}{c l}
    \hline
    Sym & Description \\ \hline
    $\vect{B}$ & Biases \\
    $\vect{I}$ & Input fmaps \\
    $\vect{W}$ & Filter weights \\
    $U$ & Convolution stride \\
    \hline
  \end{tabularx}

  \begin{tabularx}{\linewidth}{c l l}
    \hline
    Sym & Name & Description \\ \hline
    $N$ & batch size & number of in/out fmaps \\
    $C$ & channels & number of 2D in maps/filters \\
    $H$ & activations & Height of input fmap \\
    $W$ & activations & Width of input fmap \\
    $R$ & weights & Height of 2D filter \\
    $S$ & weights & Width of 2D filter \\
    $M$ & channels & Number of 2D output fmaps\\
    $E$ & activations & Height of output fmap \\
    $F$ & activations & Width of output fmap \\ 
    \hline
  \end{tabularx}

  \subsubsection{Non-linearity}
  \cgraphic{0.85}{img/tradnonlin.png}
  \cgraphic{0.85}{img/modnonlin.png}

  \subsubsection{Fully connected (FC) layer}
  Height and width of output fmaps are 1 ($E=F=1$), filters as large as input
  fmaps ($R=H,S=W$). Implementation using matrix multiplication.
  \cgraphic{0.85}{img/fcl.png}

  \subsubsection{Norm}
  \textbf{Batch normalization (BN)} normalizes activations towards mean=0, sigma=1
  based on statistics of training dataset. Put in between CONV and activation. Believed
  to be key to getting high accuracy and faster training on very deep neural nets.

  \subsubsection{Pooling}
  Reduce resolution of each channel independently. Overlapping or non-overlapping, 
  depending on stride.
  \cgraphic{0.85}{img/pooling.png}

  \subsection{Example nets}
  \subsubsection{AlexNet}
  The AlexNet has 5 conv layers, 3 FC, 61M weights, 724M MACs and uses ReLU as non-lin.
  \cgraphic{0.85}{img/alexnet.png}
   \begin{tabularx}{\linewidth}{c c c c c}
    \hline
    L\# & Filter & \#Filters & \#Chan & Stride \\ \hline
    1&$11\times11$ & 96 & 3 & 4 \\
    2&$5\times5$ & 256 & 48 & 1 \\
    3&$3\times3$ & 384 & 256 & 1 \\
    4&$3\times3$ & 384 & 192 & 1 \\
    5&$3\times3$ & 256 & 192 & 1 \\ \hline
  \end{tabularx}
  Altough layers 1 and 3 have approx. same number of MACs (120M), L3 is significantly
  more complex to compute because it has 26x more parameters (memory usage).

  \subsubsection{Summary}
  \cgraphic{1.0}{img/dnnsummary.png}
  \cgraphic{1.0}{img/moarnets.png}

  \subsection{Kernel Computations}
  Convolutions can be reshaped to form standard matrix multiplications. GPUs are 
  very good in matrix multiplications. Problem is that data is repeated.
  \cgraphic{1.0}{img/convmatrix.png}

  Likewise linear classifiers can be reshaped to form matrix multiplications.

  \subsection{Computational Transforms}
  Goal: Bitwise same result, but reduce number of operatinos. Focuses
  mostly on compute.

  \subsubsection{Strassen}
  Uses partial products to reduce the number of multiplications. Reduces matrix 
  multiplication complexity from $\mathcal{O}(N^3)$ to $\mathcal{O}(N^{2.807})$. Comes at the
  price of reduced numerical stability and requires significantly more memory. 
  Performance gain only for large matrices.

  \subsubsection{Winograd}
  1D Winograd targets convolutions instead of matrix multiply. Reduces number of multiplications
  from, e.g., 36 to 16 for 3 by 3 filter and 4 by 4 input fmap. Winograd works on small regions
  of output at a time, and therefore uses inputs repeatedly.
  \begin{outline}
    \1 Optmized computation for convolutions
    \1 Can significantly reduce multiplies
    \1 Each filter size is a different computation
  \end{outline}

  \subsubsection{FFT}
  Convolution in time domain becomes multiplication in frequency domain. Reduces convolution
  complexity from $\mathcal{O}(N_{O_2}N_{f_2})$ to $\mathcal{O}(N_{O_2}\log_2N_{O})$. Comes
  at the cost of more memory space and bandwidth.

  \subsubsection{More compute reduction approaches}
  \begin{outline}
    \1 Reduce size of operands
      \2 Floating point to fixed point
      \2 Bit-width reduction
      \2 Non-linear quantization
    \1 Reduce number of operations
      \2 Exploit activation statistics
      \2 Network pruning
      \2 Compact network architectures
  \end{outline}


  \subsection{HW-centric View}
  Most operations are based on multiply-accumulate (MAC).
  \cgraphic{1.0}{img/memloc.png}

  \subsubsection{Opportunities}
  \begin{outline}
    \1 Data reuse
    \1 Partial sum accumulation does not have to access RAM
  \end{outline}

  \subsubsection{Types of data reuse}
  Data reuse can reduce DRAM reads  of filter/fmap by up to 500x (for AlexNet)

  \textbf{Convolutional Reuse} CONV layers only: reuse activations and filter weights.
  \cgraphic{0.4}{img/convreuse.png}

  \textbf{Fmap Reuse} For CONV and FC layers: can reuse activations.
  \cgraphic{0.4}{img/fmapreuse.png}

  \textbf{Filter Reuse} CONV and FC layers with bacht size $> 1$: Reuse filter weights.
  \cgraphic{0.4}{img/filterreuse.png}

  \subsubsection{Tuning Processors for CNNs}
  Single issue, in order is the most energy efficient.
  \begin{outline}
    \1 Memory is always critical: 1 full stage for fetch, 2 full stages for load+writeback
    \1 Single stage decode
    \1 Single stage execute+WB
  \end{outline}
  \cgraphic{0.8}{img/cnnproc.png}

  \subsubsection{ISA extensions}
  \begin{outline}
    \1 HW loops and post modified LD/ST
    \1 Bit manipulations
    \1 Packed-SIMD ALU operations with dot product
    \1 Rounding and Normalization
    \1 Shuffle operations for vectors
  \end{outline}

  RISC-V3 implements all these features.
  \textbf{HW loops} mitigate counter increments and branch overheads by specifying the
  number of iterations in the loop instruction.
  
  \textbf{Bit manipulations} E.g. extract N bits starting from M from a word and extend
  with sign or find first bit set, count bumbers of 1, rotate, ...

  \textbf{Packed-SIMD} NN inference does not need 32 bit precission. Back e.g. 4 8 bit
  values into one 32 bit register and run operation on all simultaneous (SIMD=single 
  instruction, multiple data). Can run 4 8-bit MAC in one cycle.

  \subsubsection{MMUL on serial, parallel, systolic}
  MMUL = matric multiplication.

  \textbf{Serial} calculation is done by a standard CPU. Load activations and fmaps
  from memory, perform single MAC, store back in memory. Very slow.

  \textbf{Parallel} is easily implemented in GPUs. Large number of small processing units
  have their own small memory. Each performs serial computation. Better than serial
  but still slow.

  \textbf{Systolic Arrays}
  Load weights and fmaps once and propagate through the array to compute MMUL. This is how
  modern NN accelerators work (e.g. Google TPU). Con: if array is too small, partial sums
  have to be stored and the array behaves like a standard CPU.
  \cgraphic{0.8}{img/systolicarray.png}

  \textbf{MMUL in memory} Single memory cell used to store data (binary or multi-level voltage).
  By word line activation, bits get added or multiplied and result can be measured.

  % ---------------------------------------------------------------------------
  \section{Advanced Processors}
  % ---------------------------------------------------------------------------

  \subsection{Terms}
  \begin{outline}
    \1 Instruction parallelism
      \2 Number of instructions being worked on
    \1 Operation Latency
      \2 Time (in cycles) until the result of an instruction is available for use as
      an operand in a subsequent instruction
    \1 Peak IPC
      \2 maximum sustainable number of instructions executed per clock cycle
  \end{outline}

  \subsection{Instruction-Level Parallelism}
   \begin{tabularx}{\linewidth}{l X}
    IF & Instruction Fetch \\
    DE & Decode \\
    EX & Execute \\
    WB & Write Back
  \end{tabularx}
  \cgraphic{0.8}{img/pipe.png}

  \subsubsection{Superscalar Machine}
  A \textbf{superscalar machine} is able to execute multiple operations during a
  single clock cycle.
   \begin{tabularx}{\linewidth}{l l}
    {Instrucion parallelism} & $D\times N$ \\
    Operation latency & 1 \\
    Peak IPC & N
  \end{tabularx}
  \cgraphic{0.8}{img/superscalarpipe.png}

  \subsubsection{Pipeline problems}
  The pipeline can \textbf{stall} due to a raw hazard (because data dependence) or
  due to pipeline hazard (because of stuck pipeline). Here, \texttt{subf} can't proceed
  into D because \texttt{mulf} is there. 
  \cgraphic{0.8}{img/stall.png}

  An \textbf{in-order pipeline}, often written as F, D, X, W, is vulnerable to such harards.
  \textbf{Out-of-order pipeline} implements "passing" funcionality by removing structural hazards.

  \subsubsection{Instrucion-Level Parallelism (ILM)}
  ILM is a measure of the amount of inter-dependencies between instructions. Average ILP =
  number of instructions / number of cycles required. Code 1 has ILP = 1, code 2 has ILP = 3.
  All three instructions could be executed simultaneously.
  \cgraphic{0.8}{img/ilp.png}
  Takeaway: Out-of-order execution exposes more ILP.

  \subsection{Out of Order Execution}
  Or dynamic scheduling, uses a window of instructions, reorders them at runtime to exploit
  maximum pipeline usage.
  \begin{outline}
    \1 Dynamic scheduling
      \2 Totally in hardware
    \1 Fetch many instructions into instruction window
      \2 Use branch prediction to speculate past
      \2 Flush pipeline on branch misprediction
    \1 Rename to avoid false dependencies (WAW and WAR)
    \1 Execute instructions as soon as possible
      \2 Register dependencites are known
      \2 Handing memory dependencies more tricky
    \1 Commit instructions in order
      \2 Any strange happens before commig, just flush pipeline
    \1 Current machines: 100+ instructino scheduling window
  \end{outline}

  Motivation: Execute instructinos in non-sequential order but make it appear
  like sequential execution. Reduce RAW stalls, increase pipeline and funcional
  unit utilization, epose more opporunities for parallel issue.

  \subsubsection{Big picture}
  Fill instruction buffer with ready to execute instructions. \textbf{Dispatch}: first part of decode.
  Allocate slow in insn buffer, stall back-propagates to youger insns. \textbf{Issue}: second
  part of decode. Sends insns from ins buffer to execution units. Out-of-order: wait doesn't
  back-propagate to younger insns.
  \cgraphic{0.8}{img/ooobp.png}
  Issue: If multiple insns are ready, which one to choose? Most project use random.

  \subsubsection{Data dependencies}
  Definition: If insn 1's result is needed for insn 1000, there is a dependency. It is only a
  hazard, if the hardware has to deal with it. 

  True data dependency: \textbf{RAW} (read after write)
  prevents reordering. False dependency: \textbf{WAW} (write after write) and \textbf{WAR} (write after read)
  use same CPU register, but reordering is not possible because they use same reg. This can be made
  reorderable by \textbf{renaming}.

  \subsubsection{Register Renaming}
  Concept: The register names are arbitraty and only nneds to be consistent between writes.

  Approach: Every time an architected reg is written, we assign it to a physical register. Until the arch
  reg is written again, we continue to translate it to the physical reg. Leaves RAW dependencies intact.
  Free ROB at commit. Two key data structures: \texttt{map\_tbl} maps from arch register to physical reg,
  free list keeps track of allocated and free registers, implemented as a queue.
\begin{algorithm}
i.phys_i1 $\gets$ map_tbl[i.arch_i1]
i.phys_i2 $\gets$ map_tbl[i.arch_i2]
i.old_phys_o $\gets$ map_tbl[i.arch_o]

new_r $\gets$ new_phys_reg()
map_tbl[i.arch_o] $\gets$ new_r

i.phys_o $\gets$ new_r
\end{algorithm}
  
  At commit, once all older ins have committed, free register.
\begin{algorithm}
free_phys_reg(i.old_phys_o)
\end{algorithm}

  Example:
  \cgraphic{0.8}{img/renaming.png}

  Pipeline:
  \cgraphic{0.8}{img/ooopipe.png}

  \subsubsection{Reorder Buffer}
  Architectural registers and memory may only be changed if all previous operations are
  also written. Arch regs \& mem writes are always executed \textbf{in order}. This leads
  to the reorder buffer ROB. Any instructions in ROB whose RAW hazards have been satisfied
  can be dispatched. 
  \cgraphic{0.8}{img/rob.png}
  \begin{outline}
    \1 ROB managed circularly
      \2 exec bit set when ins begins execution
      \2 When an ins completes, its use bit is marked free
      \2 ptr2 is inc only if the use bit is marked free
    \1 Ins slot is candidate for execution when:
      \2 It holds a valid ins (use bit set)
      \2 It has not already started exec (exec bit clear)
      \2 Both operands are acailable (p1 and p2 set)
  \end{outline}

  Buffer size: Big enough to host average age of instruction. Approximately execution
  time of instruction. In practice 10..100.

  \subsubsection{Interrupts and Exceptions}
  \begin{outline}
    \1 Hold exection flags in pipeline unti commit point
    \1 Exceptions in earlier pipe stages override later exceptions
    \1 Inject external interrupts at commit point
    \1 If exception at commit, update cause and EPC reg, kill all stages,
    inkect halder PC into fetch stage
  \end{outline}
  \cgraphic{1.0}{img/int.png}

  \subsubsection{Commit}
  Because of exceptions, temporary storage is needed to hold results before commit
  (shadow registers and store buffers). 
  \begin{outline}
    \1 Add pd, dest, data, cause fields in ins template
    \1 Commit ins to reg file and memory in program order, buffers can be maintained 
    circularly
    \1 On exception, clear reorder buffer by resetting ptr1=ptr2 (stores must wait
    for commit before updating memory)
  \end{outline}
  \cgraphic{1.0}{img/robnew.png}
  Register file does not contain renaming tags anymore, how does the decode stage
  find the tag of a source reg? Search the "dest" field in the ROB.

  \subsubsection{Branching and Prediction}
  With that many pipeline stages, branching becoms a challenge. Next operation is
  result of branch operation, which is a heavy dependency. If high speed has to be
  achieved, multiple branches need to be speculated correctly. 
  \cgraphic{1.0}{img/insexphases.png}
  Branch prediction can be divided into:
  \begin{outline}
    \1 Target address generation (Target speculation)
      \2 Access reg: PC, general purpose reg, link reg
      \2 Perform calculation: +/- offset, autoincrement
    \1 Condition reolution (Condition speculation)
      \2 Acces reg: Condition code reg, general purpose reg
      \2 Perform calculation: Comparison of data reg(s)
  \end{outline}
  \cgraphic{1.0}{img/bpred.png}
  The branch target buffer (BTB) keeps track of taken branches. Branch predictor
  logic takes usually more area than FPU.

  In case of branch miss, ROB is rolled back.
  \begin{outline}
    \1 ROB entry holds all info for recovery/commit
      \2 All ins \& in order
      \2 Arch reg names, physical reg names, ins type
      \2 Not removed until very last thing (commit)
    \1 Operation
      \2 Fetch: insert at tail (if full, stall)
      \2 Commit: Remove from head (if not yet done, stall)
    \1 Tracking for in-order commit
      \2 Maintain appearance of in-order execution
      \2 Used also to support misprediction recovery
  \end{outline}

  \subsubsection{Trace Cache}
  Based on branch predictions, the trace cache stores blocks of instructions of streaming
  instructions considering also jumps and function calls to allow high-bandwidth fetch.

  \subsubsection{Loads and Stores}
  As already mentioned, no instruction is allowed to modify memory out of order. Nothing
  may change until ready to commit. A speculative store buffer is a structure introduced to hold
  speculative store data. 
  \begin{outline}
    \1 During decode, store buffer slot allocated in program order
    \1 Soters plit into "store adr" and "store data" micro-ops
    \1 "Store adr" execute writes tag
    \1 "Store data" execute writes data
    \1 Store commits when oldest ins and both adr and data avail
      \2 Clear speculative bit and eventually move data to cache
    \1 On store abort
      \2 Clear valid bit
  \end{outline}
  \cgraphic{0.8}{img/ldsdbuf.png}
  On load: check if adr is in store buf and/or cache. Do store buffered and cache request
  at same time for performance. If >1 entry in buf with matching adr, use the youngest entry.
  \cgraphic{0.5}{img/sdld.png}
  Another problem with loads and stores is the one above. If the address is not known, any
  load coult conflict with any previous store. We therefore can't load if a store is outstanding.
  \begin{outline}
    \1 Execute all loads and stores in program order
      \2 Load and store cannot leave ROB for ecevution until all previous loads and stores
      have completed execution
    \1 Can still execute loads and stores speculatively, and out-of-order with respect to other
    insns
    \1 Need a structure to handle memory ordering
    \1 Conservative OoO load execution
      \2 Can execute load before store, if adr known and not equal (x4!=x2)
      \2 Each load adr compared with adr of all prev. uncommitted stores
      \2 Don't exec load if any prev store adr not known
    \1 Address speculation
      \2 Guess that x4 != x2
      \2 Execute load before store adr known
      \2 Need to hold all completed but uncommitted ld/sd adr in program order
      \2 If subsequently find x4 == x2, squash ls nd all following instructions
      \2 Large penalty for inaccurate adr speculation
    \1 Memory Dependence Prediction
      \2 Guess that x4 != x2
      \2 Execute load before store adr
      \2 If later find x4 == x2, squash ld and all following ins, but mark ld ins as store-wait
      \2 Subsequent exec of same ld ins will wait for all prev sd to complete
      \2 Preiodically clear store-wait bits

  \end{outline}

  \subsubsection{Summary}
  \begin{outline}
    \1 Dynamic scheduling
      \2 Totally in hardware
      \2 Also called out-of-order execution
    \1 Fetch many instructions into instruction window
      \2 Use branch prediction to speculate past branches
      \2 Flush pipeline on branch misprediction
    \1 Rename registers to avoid false dependencies
    \1 Execute instructions as soon as possible
      \2 Register dependencies are known
      \2 Handling memory dependencies is harder
    \1 commit instructions in order
      \2 Anything strange happens before commit, flush pipeline
  \end{outline}

  \subsection{Virtual and Advanced Memory}

  \subsubsection{Virtualizing}
  \subsubsection{Address Translation}
  \subsubsection{Fast Translation}
  \subsubsection{Cache}

  % ---------------------------------------------------------------------------
  \section{Multicore Processors}
  % ---------------------------------------------------------------------------

  \cgraphic{1.0}{img/archoverview.png}

  \begin{empheq}[box=\eqbox]{equation*}
    \begin{gathered}
      U=RI
    \end{gathered}
  \end{empheq}

  \begin{outline}
    \1 
  \end{outline}

   \begin{tabularx}{\linewidth}{l c c c}
    \hline
    {} & Single & Dual & Quad \\ \hline
  \end{tabularx}



  % ---------------------------------------------------------------------------
  \section{Vector Processors, Multithreading, Virtualizing}
  % ---------------------------------------------------------------------------


  % ---------------------------------------------------------------------------
  \section{GP-GPUs}
  % ---------------------------------------------------------------------------


  % ---------------------------------------------------------------------------
  \section{Heterogeneous SoCs}
  % ---------------------------------------------------------------------------



    
\end{multicols*}

\setcounter{secnumdepth}{2}
\end{document}
