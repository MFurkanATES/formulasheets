

% ETH Systems-on-chip for Data Analytics and Machine Learning 2020
% ===========================================================================
% @Author: Noah Huetter
% @Date:   2020-02-18 17:26:28
% @Last Modified by:   noah
% @Last Modified time: 2020-04-07 10:41:46
% ---------------------------------------------------------------------------

\documentclass[a4paper, fontsize=8pt, landscape, DIV=1]{scrartcl}
\usepackage{lastpage}
\usepackage{hyperref}
\usepackage{dirtytalk}
% Include general settings and customized commands
\input{settings/general}
\input{settings/commands}

% This package makes formulas a bit more compact but less beautiful
% \usepackage{newtxtext,newtxmath}

% scala language description
\lstdefinelanguage{BNF}{%
    alsoletter={-},%
    sensitive,%
}[keywords,comments]%

\lstset{%
    basicstyle=\ttfamily,%
%    language=P4,%
    aboveskip=3mm,%
    belowskip=3mm,%
    fontadjust=true,%
%    columns=[c]fixed,%
    keepspaces=true,%
%    commentstyle=\itshape,%
    frame=single,
    keywordstyle=\bfseries,%
    captionpos=b,%
    framerule=0.3pt,%
    firstnumber=0,%
    numbersep=1.5mm,%
    numberstyle=\tiny,%
    breaklines=true,
    postbreak=\mbox{\textcolor{red}{$\hookrightarrow$}\space},
}


\lstnewenvironment{algorithm}[1][] %defines the algorithm listing environment
{   
  \lstset{ %this is the stype
    mathescape=true,
    frame=tB,
    numbers=none, 
    numberstyle=\tiny,
    basicstyle=\scriptsize, 
    keywordstyle=\color{black}\bfseries\em,
    keywords={,input, output, return, datatype, function, in, if, else, foreach, while, begin, end, } %add the keywords you want, or load a language as Rubens explains in his comment above.
    numbers=left,
    xleftmargin=.0\textwidth,
    #1 % this is to add specific settings to an usage of this environment (for instnce, the caption and referable label)
  }
}
{}

% \bibliography{semiconductordevices}
% \bibliographystyle{ieeetr}
\medmuskip=1mu

%change page style for header
\pagestyle{fancy}
\footskip 20pt

% Uncomment this line to make formulasheet ultra compact
% This removes
% - list of variables
% \newcommand{\makeultracompact}{irrelevant}
\let\makeultracompact\undefined

% Make stuff ultra compact if so desired
\ifdefined\makeultracompact
  \setlength{\parskip}{0pt}
  \setlength{\abovedisplayskip}{0pt}
  \setlength{\belowdisplayskip}{0pt}
  \setlength{\abovedisplayshortskip}{0pt}
  \setlength{\belowdisplayshortskip}{0pt}
\else
\fi
 

% Outlines configurations
\makeatletter
% the outline environment provides commands \1..\4 for
% introducing items at level 1..4, and \0 for normal paragraphs
% within the outline section.
\renewenvironment{outline}[1][]{%
  \ifthenelse{\equal{#1}{}}{}{\renewcommand{\ol@type}{#1}}%
  \ol@z%
  \newcommand{\0}{\ol@toz\ol@z}%
  \newcommand{\1}{\vspace{\dimexpr\outlinespacingscalar\baselineskip-\baselineskip}\ol@toi\ol@i\item}%
  \newcommand{\2}{\vspace{\dimexpr\outlinespacingscalartwo\baselineskip-\baselineskip}\ol@toii\ol@ii\item}%
  \newcommand{\3}{\vspace{\dimexpr\outlinespacingscalar\baselineskip-\baselineskip}\ol@toiii\ol@iii\item}%
  \newcommand{\4}{\vspace{\dimexpr\outlinespacingscalar\baselineskip-\baselineskip}\ol@toiiii\ol@iiii\item}%
}{%
  \ol@toz\ol@exit%
}
\makeatother
\def\outlinespacingscalar{0.5}
\def\outlinespacingscalartwo{0.5}

% -----------------------------------------------------------------------
\IfFileExists{../build/revision.tex}{
  \input{../build/revision.tex}
  \rhead{Compiled: \compiledate \hspace{1em} on: \hostname \hspace{1em} git-sha: \revision \hspace{1em} Noah Huetter}
}{\rhead{Noah Huetter}}

\ifdefined\makeultracompact
  \lhead{ETH Systems-on-chip for Data Analytics and Machine Learning 2020 \hspace{1em}compact version}
\else
  \lhead{ETH Systems-on-chip for Data Analytics and Machine Learning 2020}
\fi
\chead{\thepage}
\cfoot{}
\headheight 17pt \headsep 10pt
\title{ETH Systems-on-chip for Data Analytics and Machine Learning 2020}
\author{Noah Huetter}

\date{\today}
\begin{document}

\setcounter{page}{0}
\setcounter{secnumdepth}{2} %no enumeration of sections
\begin{multicols*}{4}
	\section*{Disclaimer}
	This summary is part of the lecture ``Systems-on-chip for Data Analytics and Machine Learning'' (227-0150-00L) by Prof. Dr. Luca Benini (FS20). \\[6pt]
	Please report errors to \href{mailto:huettern@student.ethz.ch}{huettern@student.ethz.ch} such that others can benefit as well.\\[6pt]	
  The upstream repository can be found at \href{https://github.com/noah95/formulasheets}{https://github.com/noah95/formulasheets}
	\vfill\null
  \columnbreak
  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \setcounter{tocdepth}{2}
  \tableofcontents
  \vfill\null
  %\columnbreak
  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\pagebreak
  \maketitle 
  \setcounter{page}{1}
  \thispagestyle{fancy}

  % ---------------------------------------------------------------------------
  \section{Architecture review, MCUs}
  % ---------------------------------------------------------------------------
  \subsection{Datacenters}
  \subsubsection{Cloud Service Models}
  \begin{outline}
    \1 Software as a Service (Saas)
      \2 Provider licenses applications to users as a service
      \2 Avoid costs of installation, maintenance, patches, ...
    \1 Platform as a Service (Paas)
      \2 Provider offers software platform for building applications
      \2 e.g. Google's App-Engine
      \2 Avoid worrying about scalability of platform
    \1 Infrastructure as a Service (Iaas)
      \2 Povider offers raw computing, storage and network
      \2 e.g. Amazon AWS
      \2 Avoid buying servers and estimating resource needs
  \end{outline}

  \subsubsection{Building blocks of moden data centers}
  \begin{outline}
    \1 Network Switch
    \1 Rack
    \1 Server Racks
    \1 Cluster Switch
  \end{outline}
  Rack of servers (so called commodity servers) consist of a modular design with
  top-of-rack switch, power, network and storage. The ToR is the link aggregate to 
  the next level.

  A data center is not just a collectino of servers, it's a "small internet". It is
  administered as a single domain that does not have to be compatible with the "outside
  world". No need for international standards.

  There exists specialized machine learning cloud hardware.

  \subsubsection{Performance Metrics}
  \begin{outline}
    \1 Throughput
      \2 Requests per second
      \2 Concurrent users
      \2 Gbytes/sec processed
    \1 Latency
      \2 Execution time
      \2 Per request latency
  \end{outline}

  \subsubsection{Tail Latency}
  Output depends on all servers finishing. Probability of a slow server $P_\text{slow}$.
  Job on $N$ servers finishes when the last server is done. Even if $P_\text{slow}$ is
  very small, $P_\text{jobFsat}$ is small if $N$ is large.
  \begin{empheq}[box=\eqbox]{equation*}
    \begin{gathered}
      P_\text{jobFast} = (1-P_\text{slow})^N
    \end{gathered}
  \end{empheq}
  \cgraphic{1.0}{img/tail.jpg}

  \subsubsection{TCO: Total Cost of Ownership}
  TCO = capital (CapEx) + operational (OpEx) expenses. CapEx: building, generators, HW.
  OpEx: Elec., repairs, insurance. Users perspective: CapEx: cost of long term leases on HW and
  services. OpEx: Pay per use cost on HW and services

  \subsubsection{Reliability}
  \begin{outline}
    \1 Failure in time (FIT)
      \2 Failures per billion hours of operation
    \1 Mean time to failure (MTTF)
      \2 Time to produce first incorrect output
    \1 Mean time to repair (MTTR)
      \2 Time to detect and repair a failure
  \end{outline}

  Steady state availability = MTTF/(MTTF + MTTR).

  \subsubsection{Multicore}
   \begin{tabularx}{\linewidth}{l c c c}
    \hline
    {} & Single & Dual & Quad \\ \hline
    Core area & $A$ & $\approx A/2$ & $\approx A/4$ \\
    Core power & $W$ & $\approx W/2$ & $\approx W/4$ \\
    Chip power & $W+O$ & $W+O'$ & $W+O''$ \\
    Core performance & $P$ & $0.9P$ & $0.8P$ \\
    Chip performance & $P$ & $1.8P$ & $3.2P$ \\ \hline
  \end{tabularx}

  \subsubsection{Amdahl's Law}
  Not all operations can run in parallel, hence speedup is limited. $f$ fraction 
  that can run in parallel.
  \begin{empheq}[box=\eqbox]{equation*}
    \begin{align}
      \text{Speedup} &= \frac{1}{(1-f)+\frac{f}{n}} & \lim_{n\to\infty}&\frac{1}{1-f+\frac{f}{n}}=\frac{1}{1-f}
    \end{align}
  \end{empheq}
  Amdahl's Law ignores power cost of $n$ cores. More cores resupts in each core being slower. 
  Parallel speedup $< n$. Seiral portion takes longer, also, inerconnect and scaling overhead.
  Fixed power budget forces slow cores, serial code quickly dominates.
  \cgraphic{1.0}{img/amdahl.png}

  \subsection{Computing Systems Performance}
  \subsubsection{Instruction Count and CPI}
  CPI: Cycles per instruction. CPU time = instruction count x CPI x clock cycle time.
  CPI is determined by program, ISA and compiler. Different instructions have diferent CPI.

  \subsubsection{Performance}
  IC: Instruction count.
  Depends on
  \begin{outline}
    \1 Algorithm: affects IC, possibly CPI
    \1 Programming language, affects IC, CPI
    \1 Compiler: affects IC, CPI
    \1 Instruction set architecture: affects IC, CPI, Tc
  \end{outline}

  \subsubsection{Power}
  \begin{empheq}[box=\eqbox]{equation*}
    \begin{gathered}
      \text{Power} = \text{Capactivice load} \times \text{Voltage}^2 \times \text{Frequency}
    \end{gathered}
  \end{empheq}

  \subsubsection{Multiprocessors}
  More than one processor per chip. Requires explicitly parallel programming.
  Hard to rogramm for performance, balance load and optimize synchronization.

  \subsection{Processor Architecture}
  CPU performance factors are determined by instruction count, CPI and cycle time.

  \subsubsection{Instruction Execution}
  Programm counter (PC) points to instruction memory to fetch instructions. Register
  numbers point to the register file to read registers. Depending on instruction class,
  use the ALU for arithmetic results, memory address calculation or branch compare. 
  Access data memory for load/store and increment PC by target address of 4 (4 byte instruction
  words).
  \cgraphic{1.0}{img/archoverview.png}

  \subsubsection{R-format instructions}
  Read two register from register file, perform arithmetic/logical operation and
  write back to register.

  \subsubsection{Load/Store Instructions}
  Read register operands, calculate address using offset, load memory to register
  or vice versa.

  \subsubsection{Branch Instructions}
  Read register, compare operands, calculate target address.

  \subsubsection{Pipelining}
  RISC-V is designed for pipelining: All instructions 32-bit, few regular instruction
  format, load/store addressing.

  \subsubsection{Hazards}
  Situations that precent starting the next instruction in the next cycle.
  \begin{outline}
    \1 Structure hazard
      \2 A required resource is busy
      \2 Fix: requires separate instruction/data memories
    \1 Data hazard
      \2 Need to wait for precious instruction to complete its data read/write
      \2 Fix: Forward data to next op without storing in register
      \2 Fix: Code scheduling to avoid stalls
    \1 Control hazard
      \2 Deciding on control action depends on precious instruction
      \2 Fix: Branch prediction
  \end{outline}

  \subsubsection{Branch Prediction}
  Predict outcome of branch and only stall if prediction is wrong. RISC-V can predict branches
  not taken. Static prediction based on typical branch behaviour. Dynamic measures actual branch
  behaviour, e.g. record recent history of each branch. Assume future behav. will continue the trend,
  when wrong, stall while re-fetching. 2-Bit predictor changes prediction only on two successive
  mispredictions.
  \cgraphic{1.0}{img/pipeline.png}

  \subsection{Exploiting Memory Hieararchy}
  Programs access a small portion of their address space at any time. Temporal locality aims at items
  that are accessed recently to be likely accessed again soon (e.g. loop). Spatial locality towards
  items near recently accessed (e.g. sequential instructions, array data).

  \subsubsection{Locality}
  Copy recently accessed (and nearby) items from disk to smaller DRAM (main memory) and from there 
  to smaller SRAM (cache attahched to CPU). If accessed data is present, \textbf{hit}, else \textbf{miss}.
  Hit ratio: hits/accesses.

  \subsubsection{Cache Memory}
  \begin{outline}
    \1 Direct Mapped Cache
      \2 Location determined by address
      \2 Direct mapped: only one choice
      \2 Store tags (high order bits of source data) and valid flag if there is data at that location
    \1 Larger block size
      \2 Store tag, index and offset
      \2 Larger blocks should reduce miss rate but larger miss penalty
    \1 Associative Cache
      \2 Allow a given block to go in any cache entry
      \2 Requires all entries to be searched at once
    \1 $n$-way set associative
      \2 Each set contains $n$ entries
      \2 Blcok number determines which set
      \2 Search all entries in a given set at once
  \end{outline}

  \subsubsection{AMAT: Average memory access time}
  AMAT = Hit time + miss rate x miss penalty

  \subsubsection{Replacement Policy}
  Direct mapped: no choice. Set associative: Prefer non-valid entry, if there is one. Otherwise,
  choose among entries in the set. Least recently used (LRU): Choose the one unused for the longest time.
  Random: Gives approx. same performance as LRU.

  % ---------------------------------------------------------------------------
  \section{(Deep) NN Workloads}
  % ---------------------------------------------------------------------------

  \subsection{Artificial Intelligence}
  John McCarthy: \say{The science and engineering of creating intelligen machines}
  \cgraphic{0.7}{img/aicloud.png}

  \subsection{Linear Classifier}
  If we want to classify an $32\times32$ image into $10$ classes we can use the 
  linear classifier:
  \begin{empheq}[box=\eqbox]{equation*}
    \begin{gathered}
      f(x,W) = Wx + b
    \end{gathered}
  \end{empheq}
   \begin{tabularx}{\linewidth}{c c l}
    \hline
    {} & Dimension & Description \\ \hline
    $W$ & $10 \times 3072$ & parameters or weights \\
    $b$ & $10 \times 1$ & bias or offset \\
    $x$ & $3072 \times 1$ & Input, e.g. 32x32x3 image pixels \\ \hline
  \end{tabularx}
  Every column in $W$ defines a plane. One side of this plane corresponds to images
  belonging to this class, the other side meaning it does not belong to this class.

  To evaluate a linear classifier, a loss function $L_i$ is defined. Given a dataset
  of examples $\{(x_i,y_i)\}_{i=1}^N$ the loss over the dataset is a sum of losses:
  \begin{empheq}[box=\eqbox]{equation*}
    \begin{gathered}
      L = \frac{1}{N}\sum_i L_i\left(f(x_i,W), y_i\right)
    \end{gathered}
  \end{empheq}
  $x_i$ image \\
  $y_i$ is (integer) label

  \subsubsection{Multiclass SVM loss}
  The SVM (support vector machine) loss function with the shorthand $s=f(x_i,W)$:
  \begin{empheq}[box=\eqbox]{equation*}
    \begin{align}
      L_i &= \sum_{j\neq y_i} \begin{cases}
          0 & s_{y_i}\geq s_j+1 \\ 
          s_j-s_{y_i}+1 & \text{otherwise}
        \end{cases} \\
      &=\sum_{j\neq y_i}\max(0, s_j-s_{y_i}+1)
    \end{align}
  \end{empheq}

  \cgraphic{0.5}{img/linlossex.png}

  \subsubsection{Other cost functions}
  \begin{empheq}[box=\eqbox]{equation*}
    \begin{align}
      \text{Softmax} &&& -\log\left(\frac{e^{sy_i}}{\sum_je^{sy_j}}\right) \\
      \text{SVM} &&& \sum_{j\neq y_i}\max(0, s_j-s_{y_i}+1) \\
      \text{Full loss} &&& \frac{1}{N}\sum_{i=1}^NL_i+R(W)
    \end{align}
  \end{empheq}

  \subsubsection{Regularization}
  Model should be "simple", so it works on test data.
  \begin{empheq}[box=\eqbox]{equation*}
    \begin{gathered}
      L(W) = L + \lambda R(W)
    \end{gathered}
  \end{empheq}

  \subsection{Training Linear Classifier}
  How do we find the best $W$ using a dataset, a score function ($s=f(x,W)$) and a
  loss function? Enter \textbf{gradiend-based optimization}: The slope in any direction
  is the dot product of the direction with the gradient.

  \begin{lstlisting}[language=Python]
while True:
  weights_grad = eval_gradient(loss_fun, data, weights)
  weights += -step_size*weights_grad\end{lstlisting}

  \subsubsection{Stochastic Gradient Descent (SGD)}
  The full sum in the cost function is expensive. Aprroximate it using a \textbf{minibatch}
  of randomly selected data (32/64/128 common).
  \begin{lstlisting}[language=Python]
while True:
  data_batch = sample_train_dat(data, 64)
  weights_grad = eval_gradient(loss_fun, data_batch, weights)
  weights += -step_size*weights_grad\end{lstlisting}

  \subsubsection{Hard cases}
  Most data are not linearly seperable.
  \cgraphic{1.0}{img/linclassfail}

  \subsection{From linear to non-linear classifiers}
  Instead of feeding the classifier with raw data, perform a non-linear transformation
  beforehand so that the data becoms linearly seperable. This is called the \textbf{Kernel Trick}.
  
  Later we will see that convolutional netowrks can work on raw data so that no
  effort has to be made on pre-processing.

  \subsection{Neural Networks}
  Consists of interconnected cells. Each cells has multiple inputs that are weighted
  and added (like a linear classifier), but then put through a non-linear activation
  function.
  \cgraphic{0.7}{img/neuralcell.png}

  Multiple cells are connected in several layers.
  \cgraphic{0.7}{img/layers.png}

  \subsection{Deep Learning}
  A deep neural network is a neural network with a lot of hidden layers.

  \subsection{Training Neural Networks}
  Neural nets are trained based on gradient descent using backpropagation. To compute the
  gradient, input values are propagated from front to back through the network and then
  the partial derivatives calculated during backpropagation.
  \cgraphic{0.7}{img/backprop.png}

   \begin{tabularx}{\linewidth}{l l}
    \hline
    \textbf{add} gate & gradient distributor \\
    \textbf{max} gate & gradient router \\
    \textbf{mul} gate & gradient switcher \\ \hline
  \end{tabularx}

  \subsubsection{Mini-batch SGD}
  \begin{enumerate}
    \item \textbf{Sample} a batch of data
    \item \textbf{Forward} propagate it through the graph, get loss
    \item \textbf{Backward} propagate to calculate the gradients
    \item \textbf{Update} the parameters using the gradient
  \end{enumerate}

  \subsubsection{Non-linearity}
  The simplest is the so called \textbf{ReLU} (rectified linear unit).
  \begin{empheq}[box=\eqbox]{equation*}
    \begin{gathered}
      f(x) = \max(0,x)
    \end{gathered}
  \end{empheq}
  \begin{outline}
    \1 Does not saturate
    \1 Very computationally efficient
    \1 Converges much faster than sigmoid/tanh in practice
    \1 Actually more biologically plausible than sigmoid
  \end{outline}


  \subsection{Convolutional Neural Networks}
  Convolutional Neural Networks are neural networks with feed forward and sparsely-connected
  with weight sharing. They are trainedusing supervised learning (training set has inputs and
  outputs, i.e., labeled). The main two layers are \textbf{convolutional layers}
  and \textbf{fully connected} layers.
  \cgraphic{1.0}{img/cnntop.png}

  The convolution layer can further be dissected into convolution, non-linearity,
  norm and pooling.

  \subsubsection{Convolution}
  The input fmap (feature map) is element-wise multiplied with filter coefficient and
  partially summed to get one entry of the output fmap. The filter is then shifted
  one entry to get the next output value and so on.
  \cgraphic{0.85}{img/conv.png}

  There can be many input channels $C$, filters $M$ and output channels $M$.
  \cgraphic{0.85}{img/batching.png}
  \begin{empheq}[box=\eqbox]{equation*}
    \begin{align}
      &O[n][m][x][y] = \text{Activation}(\vect{B}[m]) + \\
      &\sum_{i=0}^{R-1}\sum_{j=0}^{S-1}\sum_{k=0}^{C-1}\vect{I}[n][k][Ux+i][Uy+j]\times\vect{W}[m][k][i][j]
    \end{align}
  \end{empheq}
  \begin{empheq}[]{equation*}
    \begin{gathered}
      0\leq N, -\leq m < M, 0 \leq y < E, 0\leq x < F\\
      E=(H-R+U)/U, F=(W-S+U)/U
    \end{gathered}
  \end{empheq}
  \begin{tabularx}{\linewidth}{c l}
    \hline
    Sym & Description \\ \hline
    $\vect{B}$ & Biases \\
    $\vect{I}$ & Input fmaps \\
    $\vect{W}$ & Filter weights \\
    $U$ & Convolution stride \\
    \hline
  \end{tabularx}

  \begin{tabularx}{\linewidth}{c l l}
    \hline
    Sym & Name & Description \\ \hline
    $N$ & batch size & number of in/out fmaps \\
    $C$ & channels & number of 2D in maps/filters \\
    $H$ & activations & Height of input fmap \\
    $W$ & activations & Width of input fmap \\
    $R$ & weights & Height of 2D filter \\
    $S$ & weights & Width of 2D filter \\
    $M$ & channels & Number of 2D output fmaps\\
    $E$ & activations & Height of output fmap \\
    $F$ & activations & Width of output fmap \\ 
    \hline
  \end{tabularx}

  \subsubsection{Non-linearity}
  \cgraphic{0.85}{img/tradnonlin.png}
  \cgraphic{0.85}{img/modnonlin.png}

  \subsubsection{Fully connected (FC) layer}
  Height and width of output fmaps are 1 ($E=F=1$), filters as large as input
  fmaps ($R=H,S=W$). Implementation using matrix multiplication.
  \cgraphic{0.85}{img/fcl.png}

  \subsubsection{Norm}
  \textbf{Batch normalization (BN)} normalizes activations towards mean=0, sigma=1
  based on statistics of training dataset. Put in between CONV and activation. Believed
  to be key to getting high accuracy and faster training on very deep neural nets.

  \subsubsection{Pooling}
  Reduce resolution of each channel independently. Overlapping or non-overlapping, 
  depending on stride.
  \cgraphic{0.85}{img/pooling.png}

  \subsection{Example nets}
  \subsubsection{AlexNet}
  The AlexNet has 5 conv layers, 3 FC, 61M weights, 724M MACs and uses ReLU as non-lin.
  \cgraphic{0.85}{img/alexnet.png}
   \begin{tabularx}{\linewidth}{c c c c c}
    \hline
    L\# & Filter & \#Filters & \#Chan & Stride \\ \hline
    1&$11\times11$ & 96 & 3 & 4 \\
    2&$5\times5$ & 256 & 48 & 1 \\
    3&$3\times3$ & 384 & 256 & 1 \\
    4&$3\times3$ & 384 & 192 & 1 \\
    5&$3\times3$ & 256 & 192 & 1 \\ \hline
  \end{tabularx}
  Altough layers 1 and 3 have approx. same number of MACs (120M), L3 is significantly
  more complex to compute because it has 26x more parameters (memory usage).

  \subsubsection{Summary}
  \cgraphic{1.0}{img/dnnsummary.png}
  \cgraphic{1.0}{img/moarnets.png}

  \subsection{Kernel Computations}
  Convolutions can be reshaped to form standard matrix multiplications. GPUs are 
  very good in matrix multiplications. Problem is that data is repeated.
  \cgraphic{1.0}{img/convmatrix.png}

  Likewise linear classifiers can be reshaped to form matrix multiplications.

  \subsection{Computational Transforms}
  Goal: Bitwise same result, but reduce number of operatinos. Focuses
  mostly on compute.

  \subsubsection{Strassen}
  Uses partial products to reduce the number of multiplications. Reduces matrix 
  multiplication complexity from $\mathcal{O}(N^3)$ to $\mathcal{O}(N^{2.807})$. Comes at the
  price of reduced numerical stability and requires significantly more memory. 
  Performance gain only for large matrices.

  \subsubsection{Winograd}
  1D Winograd targets convolutions instead of matrix multiply. Reduces number of multiplications
  from, e.g., 36 to 16 for 3 by 3 filter and 4 by 4 input fmap. Winograd works on small regions
  of output at a time, and therefore uses inputs repeatedly.
  \begin{outline}
    \1 Optmized computation for convolutions
    \1 Can significantly reduce multiplies
    \1 Each filter size is a different computation
  \end{outline}

  \subsubsection{FFT}
  Convolution in time domain becomes multiplication in frequency domain. Reduces convolution
  complexity from $\mathcal{O}(N_{O_2}N_{f_2})$ to $\mathcal{O}(N_{O_2}\log_2N_{O})$. Comes
  at the cost of more memory space and bandwidth.

  \subsubsection{More compute reduction approaches}
  \begin{outline}
    \1 Reduce size of operands
      \2 Floating point to fixed point
      \2 Bit-width reduction
      \2 Non-linear quantization
    \1 Reduce number of operations
      \2 Exploit activation statistics
      \2 Network pruning
      \2 Compact network architectures
  \end{outline}


  \subsection{HW-centric View}
  Most operations are based on multiply-accumulate (MAC).
  \cgraphic{1.0}{img/memloc.png}

  \subsubsection{Opportunities}
  \begin{outline}
    \1 Data reuse
    \1 Partial sum accumulation does not have to access RAM
  \end{outline}

  \subsubsection{Types of data reuse}
  Data reuse can reduce DRAM reads  of filter/fmap by up to 500x (for AlexNet)

  \textbf{Convolutional Reuse} CONV layers only: reuse activations and filter weights.
  \cgraphic{0.4}{img/convreuse.png}

  \textbf{Fmap Reuse} For CONV and FC layers: can reuse activations.
  \cgraphic{0.4}{img/fmapreuse.png}

  \textbf{Filter Reuse} CONV and FC layers with bacht size $> 1$: Reuse filter weights.
  \cgraphic{0.4}{img/filterreuse.png}

  \subsubsection{Tuning Processors for CNNs}
  Single issue, in order is the most energy efficient.
  \begin{outline}
    \1 Memory is always critical: 1 full stage for fetch, 2 full stages for load+writeback
    \1 Single stage decode
    \1 Single stage execute+WB
  \end{outline}
  \cgraphic{0.8}{img/cnnproc.png}

  \subsubsection{ISA extensions}
  \begin{outline}
    \1 HW loops and post modified LD/ST
    \1 Bit manipulations
    \1 Packed-SIMD ALU operations with dot product
    \1 Rounding and Normalization
    \1 Shuffle operations for vectors
  \end{outline}

  RISC-V3 implements all these features.
  \textbf{HW loops} mitigate counter increments and branch overheads by specifying the
  number of iterations in the loop instruction.
  
  \textbf{Bit manipulations} E.g. extract N bits starting from M from a word and extend
  with sign or find first bit set, count bumbers of 1, rotate, ...

  \textbf{Packed-SIMD} NN inference does not need 32 bit precission. Back e.g. 4 8 bit
  values into one 32 bit register and run operation on all simultaneous (SIMD=single 
  instruction, multiple data). Can run 4 8-bit MAC in one cycle.

  \subsubsection{MMUL on serial, parallel, systolic}
  MMUL = matric multiplication.

  \textbf{Serial} calculation is done by a standard CPU. Load activations and fmaps
  from memory, perform single MAC, store back in memory. Very slow.

  \textbf{Parallel} is easily implemented in GPUs. Large number of small processing units
  have their own small memory. Each performs serial computation. Better than serial
  but still slow.

  \textbf{Systolic Arrays}
  Load weights and fmaps once and propagate through the array to compute MMUL. This is how
  modern NN accelerators work (e.g. Google TPU). Con: if array is too small, partial sums
  have to be stored and the array behaves like a standard CPU.
  \cgraphic{0.8}{img/systolicarray.png}

  \textbf{MMUL in memory} Single memory cell used to store data (binary or multi-level voltage).
  By word line activation, bits get added or multiplied and result can be measured.

  % ---------------------------------------------------------------------------
  \section{Advanced Processors}
  % ---------------------------------------------------------------------------

  \subsection{Terms}
  \begin{outline}
    \1 Instruction parallelism
      \2 Number of instructions being worked on
    \1 Operation Latency
      \2 Time (in cycles) until the result of an instruction is available for use as
      an operand in a subsequent instruction
    \1 Peak IPC
      \2 maximum sustainable number of instructions executed per clock cycle
  \end{outline}

  \subsection{Instruction-Level Parallelism}
   \begin{tabularx}{\linewidth}{l X}
    IF & Instruction Fetch \\
    DE & Decode \\
    EX & Execute \\
    WB & Write Back
  \end{tabularx}
  \cgraphic{0.8}{img/pipe.png}

  \subsubsection{Superscalar Machine}
  A \textbf{superscalar machine} is able to execute multiple operations during a
  single clock cycle.
   \begin{tabularx}{\linewidth}{l l}
    {Instrucion parallelism} & $D\times N$ \\
    Operation latency & 1 \\
    Peak IPC & N
  \end{tabularx}
  \cgraphic{0.8}{img/superscalarpipe.png}

  \subsubsection{Pipeline problems}
  The pipeline can \textbf{stall} due to a raw hazard (because data dependence) or
  due to pipeline hazard (because of stuck pipeline). Here, \texttt{subf} can't proceed
  into D because \texttt{mulf} is there. 
  \cgraphic{0.8}{img/stall.png}

  An \textbf{in-order pipeline}, often written as F, D, X, W, is vulnerable to such harards.
  \textbf{Out-of-order pipeline} implements "passing" funcionality by removing structural hazards.

  \subsubsection{Instrucion-Level Parallelism (ILM)}
  ILM is a measure of the amount of inter-dependencies between instructions. Average ILP =
  number of instructions / number of cycles required. Code 1 has ILP = 1, code 2 has ILP = 3.
  All three instructions could be executed simultaneously.
  \cgraphic{0.8}{img/ilp.png}
  Takeaway: Out-of-order execution exposes more ILP.

  \subsection{Out of Order Execution}
  Or dynamic scheduling, uses a window of instructions, reorders them at runtime to exploit
  maximum pipeline usage.
  \begin{outline}
    \1 Dynamic scheduling
      \2 Totally in hardware
    \1 Fetch many instructions into instruction window
      \2 Use branch prediction to speculate past
      \2 Flush pipeline on branch misprediction
    \1 Rename to avoid false dependencies (WAW and WAR)
    \1 Execute instructions as soon as possible
      \2 Register dependencites are known
      \2 Handing memory dependencies more tricky
    \1 Commit instructions in order
      \2 Any strange happens before commig, just flush pipeline
    \1 Current machines: 100+ instructino scheduling window
  \end{outline}

  Motivation: Execute instructinos in non-sequential order but make it appear
  like sequential execution. Reduce RAW stalls, increase pipeline and funcional
  unit utilization, epose more opporunities for parallel issue.

  \subsubsection{Big picture}
  Fill instruction buffer with ready to execute instructions. \textbf{Dispatch}: first part of decode.
  Allocate slow in insn buffer, stall back-propagates to youger insns. \textbf{Issue}: second
  part of decode. Sends insns from ins buffer to execution units. Out-of-order: wait doesn't
  back-propagate to younger insns.
  \cgraphic{0.8}{img/ooobp.png}
  Issue: If multiple insns are ready, which one to choose? Most project use random.

  \subsubsection{Data dependencies}
  Definition: If insn 1's result is needed for insn 1000, there is a dependency. It is only a
  hazard, if the hardware has to deal with it. 

  True data dependency: \textbf{RAW} (read after write)
  prevents reordering. False dependency: \textbf{WAW} (write after write) and \textbf{WAR} (write after read)
  use same CPU register, but reordering is not possible because they use same reg. This can be made
  reorderable by \textbf{renaming}.

  \subsubsection{Register Renaming}
  Concept: The register names are arbitraty and only nneds to be consistent between writes.

  Approach: Every time an architected reg is written, we assign it to a physical register. Until the arch
  reg is written again, we continue to translate it to the physical reg. Leaves RAW dependencies intact.
  Free ROB at commit. Two key data structures: \texttt{map\_tbl} maps from arch register to physical reg,
  free list keeps track of allocated and free registers, implemented as a queue.
\begin{algorithm}
i.phys_i1 $\gets$ map_tbl[i.arch_i1]
i.phys_i2 $\gets$ map_tbl[i.arch_i2]
i.old_phys_o $\gets$ map_tbl[i.arch_o]

new_r $\gets$ new_phys_reg()
map_tbl[i.arch_o] $\gets$ new_r

i.phys_o $\gets$ new_r
\end{algorithm}
  
  At commit, once all older ins have committed, free register.
\begin{algorithm}
free_phys_reg(i.old_phys_o)
\end{algorithm}

  Example:
  \cgraphic{0.8}{img/renaming.png}

  Pipeline:
  \cgraphic{0.8}{img/ooopipe.png}

  \subsubsection{Reorder Buffer}
  Architectural registers and memory may only be changed if all previous operations are
  also written. Arch regs \& mem writes are always executed \textbf{in order}. This leads
  to the reorder buffer ROB. Any instructions in ROB whose RAW hazards have been satisfied
  can be dispatched. 
  \cgraphic{0.8}{img/rob.png}
  \begin{outline}
    \1 ROB managed circularly
      \2 exec bit set when ins begins execution
      \2 When an ins completes, its use bit is marked free
      \2 ptr2 is inc only if the use bit is marked free
    \1 Ins slot is candidate for execution when:
      \2 It holds a valid ins (use bit set)
      \2 It has not already started exec (exec bit clear)
      \2 Both operands are acailable (p1 and p2 set)
  \end{outline}

  Buffer size: Big enough to host average age of instruction. Approximately execution
  time of instruction. In practice 10..100.

  \subsubsection{Interrupts and Exceptions}
  \begin{outline}
    \1 Hold exection flags in pipeline unti commit point
    \1 Exceptions in earlier pipe stages override later exceptions
    \1 Inject external interrupts at commit point
    \1 If exception at commit, update cause and EPC reg, kill all stages,
    inkect halder PC into fetch stage
  \end{outline}
  \cgraphic{1.0}{img/int.png}

  \subsubsection{Commit}
  Because of exceptions, temporary storage is needed to hold results before commit
  (shadow registers and store buffers). 
  \begin{outline}
    \1 Add pd, dest, data, cause fields in ins template
    \1 Commit ins to reg file and memory in program order, buffers can be maintained 
    circularly
    \1 On exception, clear reorder buffer by resetting ptr1=ptr2 (stores must wait
    for commit before updating memory)
  \end{outline}
  \cgraphic{1.0}{img/robnew.png}
  Register file does not contain renaming tags anymore, how does the decode stage
  find the tag of a source reg? Search the "dest" field in the ROB.

  \subsubsection{Branching and Prediction}
  With that many pipeline stages, branching becoms a challenge. Next operation is
  result of branch operation, which is a heavy dependency. If high speed has to be
  achieved, multiple branches need to be speculated correctly. 
  \cgraphic{1.0}{img/insexphases.png}
  Branch prediction can be divided into:
  \begin{outline}
    \1 Target address generation (Target speculation)
      \2 Access reg: PC, general purpose reg, link reg
      \2 Perform calculation: +/- offset, autoincrement
    \1 Condition reolution (Condition speculation)
      \2 Acces reg: Condition code reg, general purpose reg
      \2 Perform calculation: Comparison of data reg(s)
  \end{outline}
  \cgraphic{1.0}{img/bpred.png}
  The branch target buffer (BTB) keeps track of taken branches. Branch predictor
  logic takes usually more area than FPU.

  In case of branch miss, ROB is rolled back.
  \begin{outline}
    \1 ROB entry holds all info for recovery/commit
      \2 All ins \& in order
      \2 Arch reg names, physical reg names, ins type
      \2 Not removed until very last thing (commit)
    \1 Operation
      \2 Fetch: insert at tail (if full, stall)
      \2 Commit: Remove from head (if not yet done, stall)
    \1 Tracking for in-order commit
      \2 Maintain appearance of in-order execution
      \2 Used also to support misprediction recovery
  \end{outline}

  \subsubsection{Trace Cache}
  Based on branch predictions, the trace cache stores blocks of instructions of streaming
  instructions considering also jumps and function calls to allow high-bandwidth fetch.

  \subsubsection{Loads and Stores}
  As already mentioned, no instruction is allowed to modify memory out of order. Nothing
  may change until ready to commit. A speculative store buffer is a structure introduced to hold
  speculative store data. 
  \begin{outline}
    \1 During decode, store buffer slot allocated in program order
    \1 Soters plit into "store adr" and "store data" micro-ops
    \1 "Store adr" execute writes tag
    \1 "Store data" execute writes data
    \1 Store commits when oldest ins and both adr and data avail
      \2 Clear speculative bit and eventually move data to cache
    \1 On store abort
      \2 Clear valid bit
  \end{outline}
  \cgraphic{0.8}{img/ldsdbuf.png}
  On load: check if adr is in store buf and/or cache. Do store buffered and cache request
  at same time for performance. If >1 entry in buf with matching adr, use the youngest entry.
  \cgraphic{0.5}{img/sdld.png}
  Another problem with loads and stores is the one above. If the address is not known, any
  load coult conflict with any previous store. We therefore can't load if a store is outstanding.
  \begin{outline}
    \1 Execute all loads and stores in program order
      \2 Load and store cannot leave ROB for ecevution until all previous loads and stores
      have completed execution
    \1 Can still execute loads and stores speculatively, and out-of-order with respect to other
    insns
    \1 Need a structure to handle memory ordering
    \1 Conservative OoO load execution
      \2 Can execute load before store, if adr known and not equal (x4!=x2)
      \2 Each load adr compared with adr of all prev. uncommitted stores
      \2 Don't exec load if any prev store adr not known
    \1 Address speculation
      \2 Guess that x4 != x2
      \2 Execute load before store adr known
      \2 Need to hold all completed but uncommitted ld/sd adr in program order
      \2 If subsequently find x4 == x2, squash ls nd all following instructions
      \2 Large penalty for inaccurate adr speculation
    \1 Memory Dependence Prediction
      \2 Guess that x4 != x2
      \2 Execute load before store adr
      \2 If later find x4 == x2, squash ld and all following ins, but mark ld ins as store-wait
      \2 Subsequent exec of same ld ins will wait for all prev sd to complete
      \2 Preiodically clear store-wait bits

  \end{outline}

  \subsubsection{Summary}
  \begin{outline}
    \1 Dynamic scheduling
      \2 Totally in hardware
      \2 Also called out-of-order execution
    \1 Fetch many instructions into instruction window
      \2 Use branch prediction to speculate past branches
      \2 Flush pipeline on branch misprediction
    \1 Rename registers to avoid false dependencies
    \1 Execute instructions as soon as possible
      \2 Register dependencies are known
      \2 Handling memory dependencies is harder
    \1 commit instructions in order
      \2 Anything strange happens before commit, flush pipeline
  \end{outline}

  \subsection{Virtual and Advanced Memory}

  \subsubsection{Virtualizing}
  Problem when running multiple apps and the OS: How do they share memory?
  \textbf{Goal}: Each application thinks it has infinite memory. 
  \begin{outline}
    \1 Treat memory as a "cache" and store overflowed blocks in "swap" space on disk
    \1 Add a level of indirection (address translation)
  \end{outline}

  Virutual Memory (VM):
  \begin{outline}
    \1 Level of indirection
    \1 App generated adresses are virtual addr. (VA)
      \2 Each proc can act as if has its own $2^N$ B adr space
    \1 Mem accessed usin physical adr (PA)
    \1 VA translated to PA at some coarse granularity (page)
    \1 OS contorls VA to PA mapping for itself and all other proc
    \1 Logically: translate performed before ecery insn fetch, ld, sd
    \1 Physicallu: HW acceleration remoces translation overhead
  \end{outline}
  \cgraphic{1}{img/virtmeml.png}

  \begin{outline}
    \1 programs use virtual adr (VA)
      \2 VA size (V) aka V-bit ISA (e.g., 64-bit x86)
    \1 Memory uses physical adr (PA)
      \2 $2^M$ is most phy mem machine supports
    \1 VA $\to$ PA at page granulariy (VP$\to$PP)
      \2 Mapping need not preserve contiguity
      \2 VP need not be mapped to any PP
      \2 Unmapped VPs live on disk (swap) or nowhere (if not yet touched)
  \end{outline}

  Uses: \\
  \textbf{Isolation}: Each app thinks it has its own memory. Apps can't address another program's
  memory.\\
  \textbf{Protection}: Each page with a r/w/ex permission set by OS. Enforced by hardware.\\
  \textbf{Inter-process com}: Map sampe phys pages into multiple virtual adr spaces. Or share files
  via UNIX \lstinline[language=C]!mmap()!

  \subsubsection{Address Translation}
  VA$\to$PA mapping called address translation. Split VA into virtual page number (VPN) and
  page offset (POFS). Translace VPN into physical page number (PPN). POFS is not translated.
  Translation is done in software (for now) with HW acceleration. OS has a page table (PT)
  that maps VPs to PPs ot to disk (swap) adr. Translation is table lookup.
  \cgraphic{1}{img/adrtranslate.png}

  Replacements and writes:
  \begin{outline}
    \1 To reduce page fault rate, prefer least recently used (LRU) replacement
      \2 Reference bit (aka use bit) in PT set to 1 on access to page
      \2 Periodically cleared to 0 by OS
      \2 A page with reference but = 0 has not been used recently
    \1 Disk writes take millions of cycles
      \2 Block at once, not individual locations
      \2 Write through is impractical
      \2 Use write-back
      \2 Dirty bit in PT set when page is written
  \end{outline}

  \textbf{Problem}: Page tables can get big. One solution is multi-level page tables. A 
  trie of pages, lowest level tables hold PTEs. VPN is split into $N$ levels.
  \cgraphic{0.8}{img/hierpt.png}

  \subsubsection{Fast Translation}
  Problem with virtual memory is that a ld/sd op would first have to access PT then
  the actual memory access. But this access has good locality. Therefore use a fast
  cache of PTEs within the CPU called \textbf{Translation Look-aside Buffer (TLB)}
  Typically 16-512 PTEs , 0.5-1 cycle for hit, 10-100 for miss, 0.01-1\% miss rate.
  Misses can be handled by HW or SW.
  \cgraphic{1}{img/tlb.png}

  TLB misses load PTE from memory and retry. If page not in memory, OS handles fetching and update
  of table. TLB miss indicates page present, but PTE not in TLB or page not present. Must recognize TLB
  miss before destination register overwritten $\to$raise exception.
  \cgraphic{0.7}{img/tlbcpu.png}

  \subsubsection{Summary}
  TLB should "cover" the size of on-chip caches.
   \begin{tabularx}{\linewidth}{l X}
    \hline
    Abbr. & Text \\ \hline
    VM & Virtual Memory \\
    PA & Physical address \\
    VP & Virtual page \\
    PP & Physical page \\
    VPN & Virtual page number \\
    POFS & Page offset \\
    PPN & Physical page number \\
    PT(E) & Page table (entry)\\
    LRU & Least recently used \\
    TLB & Translation Look-aside Buffer\\
    \hline
  \end{tabularx}

  \vfill\null
  \columnbreak
  \subsection{Cache}
  \subsubsection{Direct mapped}
  In a direct-mapped cache structure, the cache is organized into multiple sets with a single 
  cache line per set. Based on the address of the memory block, it can only occupy a single cache 
  line.
  \cgraphicc{1.0}{img/dmcache.png}{Direct mapped \$}
  \begin{outline}
    \1 Place block in cache
      \2 Set determined by idx bit derived from mem adr
      \2 Place mem block and set tag
      \2 If cache line previousy occupied, new data replaces
    \1 Search word in cache
      \2 Set is identified by idx bit of adr
      \2 If tag match hit, else miss
    \1 Advantages
      \2 Placement is power efficient as it avoids search
      \2 Place and replace is simple
      \2 Cheap HW as only one tag needs to be checked at a time
    \1 Disadvantages
      \2 Lower cache hit rate
  \end{outline}

  \vfill\null
  \columnbreak
  \subsubsection{Fully associative}
  n a fully associative cache, the cache is organized into a single cache set with multiple 
  cache lines. A memory block can occupy any of the cache lines.
  \cgraphicc{1.0}{img/facache.png}{Fully associative \$}
  \begin{outline}
    \1 Place block in cache
      \2 Mem block can be placed if valid bit = 0
      \2 If cache is completely occupied, a block is evicted and mem is placed in that line
    \1 Search word in cache
      \2 Tag field of mem adr is compared with tag in all cache lines. If match, block is present in cache
      and is a hit.
      \2 Based on offset, byte is selected and returned
    \1 Advantages
      \2 Any mem block can be placed in any cache line, full cache utilization
      \2 better hit rate
    \1 Disadvantages
      \2 placement is slow and power inefficient
      \2 High cost due to comparison HW
  \end{outline}

  \vfill\null
  \columnbreak
  \subsubsection{Set associative}
  Set-associative cache is a trade-off between direct-mapped cache and fully associative cache.
  A set-associative cache can be imagined as a ($n\times m$) matrix. The cache is divided into $n$
  sets and each set contains $m$ cache lines. A memory block is first mapped onto a set and then
  placed into any cache line of the set. The range of caches from direct-mapped to fully associative 
  is a continuum of levels of set associativity. (A direct-mapped cache is one-way set-associative 
  and a fully associative cache with $m$ cache lines is $m$-way set-associative.) 
  \cgraphicc{1.0}{img/sacache.png}{Set associative \$}
  \begin{outline}
    \1 Place block in cache
      \2 Set is determined by index bits derived from memory adr
      \2 Place memory block and store tag with associated set
      \2 If cache line occupied, new data replaces old
    \1 Search word in cache
      \2 Set is determined by index bit from memory adr
      \2 Compare tag bits with all tags of selected set. If match then hit, else miss
    \1 Advantages
      \2 Placement is trade-off between direct map and fully associative
      \2 Offers flexibility of replacement algos if a cache miss occurs
    \1 Disadvantages
      \2 Placement will not effectively use all availabe cache lines
  \end{outline}



  \vfill\null
  \columnbreak
  % ---------------------------------------------------------------------------
  \section{Multicore Processors}
  % ---------------------------------------------------------------------------
  \subsection{Today's Chips}
  \begin{outline}
    \1 Multiple Cores per chip yields higher performance
    \1 Multiple memory hierarchy
      \2 L1 private to processor
      \2 L2 private to processor
      \2 L3 distributed and shared across procesor
    \1 Up to 200W TPU
  \end{outline}

  \subsection{Classification}
  \cgraphic{1.0}{img/mchier.png}
  \begin{outline}
    \1 On energy constrained devices with varying task load big LITTLE architecture is best
    \1 Task scheduling is a big deal for heterogenous architectures in orde to
    decide on which hardware a task runs
  \end{outline}

  \cgraphicc{0.8}{img/mchomo.png}{Skylake Intel 14nm, high performance, GPU, IO}
  \cgraphicc{0.5}{img/amdchiplet.png}{AMD chiplet, cores on different dies, one main die for IO, L3\$}

  \subsection{Parallel Architectures}
  \cgraphic{0.8}{img/comnet.png}
  
  \subsubsection{Shared Memory}
  \cgraphic{0.8}{img/sharedmem.png}
  \begin{outline}
    \1 To commuicate from one proc to the other, write to specific memory
    \1 Key problems
      \2 Cherence problem: copy to all processors local cache
      \2 Memory consistency issue: modify all copies
      \2 Synchronization problem: Only read if data is ready
    \1 Two variants
      \2 Physical shared (SMP)
      \2 Distributed shared (DSM)
  \end{outline}

  \subsubsection{Symmetric Multi-Processors SMP}
  Memory: Centralized with uniform access time \textbf{UMA} and bus interconnect, IO
  \cgraphic{0.8}{img/smp.png}

  \subsubsection{Distributed Shared Memory DSM}
  Nonuniform acces time \textbf{NUMA} and scalable interconnect (distributed memory)
  \cgraphic{0.8}{img/dsm.png}

  \subsubsection{Message Passing}
  Is a communication model using communication primitives, e.g, send, receive library calls.
  Message passing can be built on top of shared memory and vice versa.
  \cgraphic{0.8}{img/mp.png}

  \begin{outline}
    \1 DMA reads from memory, wraps message, sends on network
    \1 DMA listens on network parses message, writes to memory and inform processor
    \1 There is no linear speedup due to sequential program and communication overhead
    \1 Latency hiding: do something while waiting
    \1 Easier scalable
  \end{outline}
  \cgraphic{0.8}{img/mpdma.png}

  \subsubsection{Shared memory programming}
  \begin{outline}
    \1 OpenMP compiles code to multiple independant threads
    \1 Process consists of multiple parallel threads
    \1 Two threads can have same variable name, but allocated in two different private space
    \1 If allocated in shared mem, all procs can read/write
    \1 OS decides which thread to run
    \1 Fork/join parallelism
  \end{outline}
  \cgraphic{0.8}{img/seqpara.png}

  \subsubsection{Pragma}
  Use pragmas to tell compiler infos about data and parallelism.
  \begin{lstlisting}[language=C]
int main() {
  #pragma omp parallel {
    printf("Hello World\n");
  }
}\end{lstlisting}
  \begin{lstlisting}[language=C]
int main() {
  omp_parallel_start(&parfun, ...);
  parfun();
  omp_parallel_end();
}
int parfun(...)
{
  printf("Hello world\n");
}\end{lstlisting}

  \begin{outline}
    \1 Use \lstinline[language=C]!shared()! and \lstinline[language=C]!private()! to indicate variables
    \1 For loop can be executed in parallel using \lstinline[language=C]!#pragma omp for!
  \end{outline}

  \begin{lstlisting}[language=C]
#pragma omp for schedule(static)\end{lstlisting}
  Static: Usefule for simple, regular loops with equal duration iterations. 
  \begin{lstlisting}[language=C]
#pragma omp for schedule(dynamic)\end{lstlisting}
  Dynamic: A thread is generated for a single iteration. Thds are moved to work queue, work is fetched from
  queue dynamically.

  \begin{outline}
    \1 Fine-grain parallelism
      \2 Good for load balancing
      \2 Small amounts of computational work between parallelism computaiton stages
      \2 Low computaiton to parallelization ratio, high overhead
    \1 Coarse-grain parallelism
      \2 Harder to load balance efficiently, but
      \2 Large amounts of computational work between parallelism computation stages
      \2 High computation to parallelization ratio, low overhead
  \end{outline}

  \begin{lstlisting}[language=C]
#pragma omp for schedule(dynamic, 2)\end{lstlisting}
  Reduces overhead by making bigger chunks (always queue 2 iterations on same core).
  
  \begin{lstlisting}[language=C]
#pragma omp critical\end{lstlisting}
  Critical section: a portion of code that only one thread at a time may execute. Stops race concidions.
  
  \begin{lstlisting}[language=C]
#pragma omp reduction(+:area)\end{lstlisting}
  Instructs the compiler to create private copies of \texttt{area} for every thread. At the end of loop, 
  partial sums are combined on the shared variable.


  \subsection{Threading \& Shared Memory Programming Model}
  \begin{outline}
    \1 Software thread
      \2 Per thread state: PC \& registers
      \2 Shared state: global variables, heap
      \2 Different registers in memory
    \1 OS manages threads
      \2 Thread scheduling and time multiplexing
  \end{outline}

  \subsubsection{Shared Memory}
  \begin{outline}
    \1 Programmer explicitly creates multiple threads
    \1 All ld, sd to a single share memory space
    \1 A thread switch can occur at any time
      \2 Pre-emptive multithreading by OS
    \1 Issues
      \2 Cache coherency
      \2 Synchronization
      \2 Memory consistency
  \end{outline}

  \subsubsection{Shared Cache}
  \cgraphic{1.0}{img/sharedcache.png}
  No cache coherency problems because only one cache. But shared cache becomes
  bottleneck.

  \subsubsection{Private Cache}
  \cgraphic{1.0}{img/privcache.png}
  Faster access, lower power. Coherency problem: Snooping solves this.

  \subsubsection{Snooping}
  \begin{outline}
    \1 Send all requests for data to all procs
    \1 Procs snoop to see if they have a copy and respond accordingly
    \1 Requires broadcast, since caching information is at processors
    \1 Works well with bus
    \1 Dominates for small scale machines
    \1 Write invalidates all caches and main memory
  \end{outline}

  % --- Week 5+ ---------------------------------------------------------------

  \subsubsection{Optimized Snoop with L2\$}
  Snooping on L2 does not affect CPU-L1 bandwidth. Only small amount of invalidations
  is forwarded to L1: Cache lines included in L1 that need to be invalidated. Most
  is filtered at L2 level. Inclusion property: We have to make sure, that if an entry 
  is in L1, it must bee in L2.
  \cgraphic{0.8}{img/optsnoop.png}

  \begin{outline}
    \1 Improves performance and complexity
    \1 New interaction between L1 and L2 (Inclusion property)
    \1 Reduces traffic of invalidates to L1
  \end{outline}

  \textbf{Write-back cache}: Modified chache line is only written back to memory, if cache
  line is removed.

  \textbf{Write-through cache}: Modified cache line is immediately written back to main
  memory.

  \subsubsection{Multi-Core Shared Cache}
  Private L1\$, shared L2\$, bus between L1s and single L2 controller, snooping-based
  coherence between L1s. Todays CPUs have shared L3.
  \cgraphic{0.5}{img/mcsco.png}

  \subsubsection{Multi-Core Cache}
  Private L1, shared but physically distributed L2. Bus connecting the four L1s
  and four L2 bankds. Snooping-based coherence between L1s.
  \cgraphic{0.5}{img/mcco.png}

  \subsubsection{False Sharing}
  A cache line contains more than one word. Cache-coherence is done at the line-level
  and not word-level. Suppose $M_1$ writes $w_1$ and $M_2$ writes $w_2$ and both
  have same line address, what can happen?

  \subsubsection{Performance of Symmetric Multiprocessors SMP}
  Cache performance is combination of 
  \begin{outline}
    \1 Uniprocessor cache miss traffic
    \1 Traffic caused by communication
      \2 Results in invalidations and subsequent cache misses
    \1 Coherence misses
      \2 Also called communication miss
      \2 4th C of cache misses along with Compulsory, Capacity \& Conflict
  \end{outline}

  \subsubsection{Coherency miss}
  Also called communication miss
  \begin{outline}
    \1 True sharing misses areise from the communication of data through the cache 
    coherence mechanism
    \1 False shring misses when a line is invalidated because some word in the line,
    other than the one being read, is written into
  \end{outline}
  Example: x1, x2 in same cache line. P1 and P2 both read x1 and x2 before.
  \cgraphic{1}{img/coherencymiss}


  \subsubsection{Directory Cache Protocol}
  Every memory line has associated directory information that
  \begin{outline}
    \1 Keeps track of copies of cached lines and their states
    \1 On a miss, find directory entry, look it up and communicate only with the
    nodes that have copies if necessary
    \1 In scalable networks, communicate with directory and copies is through network
    transactions
  \end{outline}
  Many alternatives for organizing directory information.

  Assumptions: Reliable network, Fifo message delivery between any given source-destination
  pair.
  \cgraphic{1}{img/directorycache.png}

  Direcotry bit is set to 1, if the selected CPU has line in its own cache. 1 bit per CPU.
  On write, if multiple bits are set, send invalidates \textbf{only} to CPUs that have
  the line in cache. Broadcast becomes multicast. 

  Each cache line has 4 possible states:  
  \begin{outline}
    \1 C-invalid (Nothing): The accessed data is not resident in the cache
    \1 C-shared (Sh): Accessed data is resident in the cache, and possibly also cached at
    other sites. Data in memory is valid
    \1 C-modified (Ex): Data exclusiverly resident in this cache, and has been modified. Memory
    does not have most up-to-date data
    \1 C-transient (Pending): Data is in a transient state (e.g., site has just issued a
    protocol request, but has not received the corresponding reply)
  \end{outline}

  Each cache line has 4 possible directory states:
  \begin{outline}
    \1 R(dir): Mem line is shared by the sites specified in dir, data in mem is valid
    in this state. If dir is empty, memory line is not cached by any site
    \1 W($id$): Mem line is exlusively cached at site $id$, and has been modified at that
    site. Mem does not have the most up-to-date data
    \1 TR(dir): The mem line is in a transient state, waiting for the ack to the invalidation req 
    that the home site has issued
    \1 TW($id$): The mem line is in a transient state waiting for a line exclusively cached at site
    $id$ (i.e., in C-modified state) to make mem line at home site up-to-date
  \end{outline}

  \subsubsection{Alternatives to cache coherence}
  Coherence keeps cahces "coherent", Load returns the most recent stored value by any processor
  and thus keeps caches transparent to software.

  Alternatives:
  \begin{outline}
    \1 No caching of shared data (slow)
    \1 Requiring software to explicitly "flush" data (hard to use), use some new instrucitons
    \1 Message passing (programming witout shared memory), used in clusters of machines for 
    high-performance computing
  \end{outline}

  Directory-based coherence protocol scales well, perhaps to 1000s of cores.

  \subsection{Shared memory issue: Synchronization}
  How to regulate access to shared data? How to implement "locks"?

  \subsubsection{Problem}
  Thread level parallelism on shared data must be synchronized.

  \subsubsection{Synchronization}
  Regulate access to shared data (mutual exclusion). Low-level primitive: lock
  \begin{outline}
    \1 \texttt{acquire} and \texttt{release}
    \1 Region between \texttt{acquire} and \texttt{release} is a critical section
    \1 Must interleave \texttt{acquire} and \texttt{release}
    \1 Interfering \texttt{acquire} will block
  \end{outline}

  \subsubsection{Strawman Lock (incorrect)}
  Spin lock: software lock implementation
  \begin{lstlisting}[language=C]
acquire(lock): while (lock != 0) {} lock = 1;\end{lstlisting}
  In assembly:
  \begin{lstlisting}[language={[x86masm]Assembler}]
A0: ld r6 <- 0(&lock)
A1: bnez r6,A0
A2: addi r6 <- 1,r6
A3: st r6 -> 0(&lock)\end{lstlisting}

  \textbf{Problem}: Can and will go wrong:
  \cgraphic{1.0}{img/falselock.png}

  \subsubsection{Correct Spin Lock}
  Compare and Swap: Single atomic operation, e.g. \texttt{cas r3 <- r1,r2,0(\&lock)} performs atomically:
  \begin{lstlisting}[language={[x86masm]Assembler}]
ld r3 <- 0(&lock)
if r3 == r2:
  st r1 -> 0(&lock)\end{lstlisting}
  New acquire sequence:
  \begin{lstlisting}[language={[x86masm]Assembler}]
A0: cas r3 <- 1,0,0(&lock)
A1: bnez r3,A0\end{lstlisting}
  Ensures that lock is held by at most one thread.

  \subsubsection{RICS implementation}
  CAS: Load+branch+store in one instruction is not very "RISC". Broken up into 
  micro-ops: load-link and store-conditional pairs.
  \begin{lstlisting}[language={[x86masm]Assembler}]
label:
  load-link r1 <- 0(&lock)
  // potentially other insns
  store-conditional r2 -> 0(&lock)
  vranch0not0zero label // check for failure
A1: bnez r3,A0\end{lstlisting}
  On \texttt{load-link}, processor remembers address, looks for writes by other processors,
  if write is detected, next \texttt{store-conditional} will fail, sets failure condition.

  Used by ARM, PowerPC, MIPS, Inanium.

  \subsubsection{Fine-grain locks}
  Multiple locks, one per record. Fast: critical sections to different records, can
  proceed in parallel. But easy to make mistakes. 

  \textbf{Deadlock}: If two threads need mutliple locks, they acquire one lock
  and the other thread acquires the other lock, both threads are stuck in a deadlock.
  \cgraphic{0.8}{img/deadlock.png}

  \subsubsection{Coffman Conditions for Deadlock}
  4 necessary conditions: Break any one of these conditions to get deadlock freedom.
  \begin{outline}
    \1 Mutual exclusion
    \1 Hold and wait
    \1 No preemption
    \1 Circular waiting
  \end{outline}

  \subsubsection{Lock order}
  Always acquire multiple locks in same order, yet another thing to keep in mind when
  programming.
  \cgraphic{0.8}{img/lockorder.png}

  \subsubsection{More problems..}
  What if:
  \begin{outline}
    \1 Some actions require 1 or 2 locks and others require all of them?
    \1 There are locks for global variables? Then should operations grab this lock?
  \end{outline}

  Research: Transactional memory (TM). Goals:
  \begin{outline}
    \1 Programming simplicity of coarse-grain locks
    \1 Higher concurrency (parallelism) of fine-grain locks
    \1 Lower overhead than lock acquisition
  \end{outline}

  Idea: No locks, just shared data, optimistic (speculative) concurrency. Execute
  critical section speculatively, abort on conflicts. Detect conflicts via coherence protocol.
  "Better to ask for forgiveness than permission".

  \subsection{Memory consistency models}
  How to keep programmer sane while letting hardware optimize? How to reconcile
  shared memory with compiler optimiztions, store buffers, and out-of-order exec?
  \cgraphic{0.8}{img/memcons.png}
  Observation: If write are immediately, it is impossible that both if-statements 
  evaluate to true. But what if write invalidate is delayed?

  \subsubsection{Sequential Consistency (SC)}
  A multiprocessor is sequentially consistent, if the result of any execution is the same
  as if the (mem) ops of all procs were executed in seq. order. All procs see all ld and sd
  happening in same order.

  Implementation:
  \begin{outline}
    \1 Delay completino of any mem access until all invalidations caused by that access
    are completed
    \1 Delay next mem access until precious one is completed
  \end{outline}

  Enforcing SC can be quite expensive. Solutions: Exploit latency-hiding techniques,
  employ relaxed consistency.

  \subsubsection{Hih-Performance SC}
  Load queue records all speculative loads. Bus write/upgrades are checked againts LQ.
  Any mathicn load gets marked for replay. At commit, loads are checked and replayed if necesary.
  \cgraphic{0.8}{img/hpsc.png}

  \subsubsection{Relaxed concistency models}
  Key insight: only synchronizatino references need to be ordered. Hence, relax memory
  for all other references. Require programmer to label synchronization references.
  Ofte: Fence ops cause pipeline drain in moden OOO machine.

  % ---------------------------------------------------------------------------
  % Chapter 6 slide ff
  \section{Vector Processors, Multithreading, Virtualizing}
  % ---------------------------------------------------------------------------
  Performance beyond single thred instruction level parallelism: Data level
  parallelism: Perform identical operations on data, and lots of data.

  \subsection{Multithreading (MT)}
  Performing multiple threads of execution in parallel: Replicate registeres, PC, etc.
  Fast switching between threads. \textbf{Fine-Grain MT} switch threads after each cycle, 
  interleave instruction execution, if one thread stalls, others are executed.
  \textbf{Coarse-gran MT} only switch on long stall (eg. L2\$-miss). Simplifies
  hardware, but doesn't hide short stalls.

  \subsubsection{Simultaneous MT (SMT)}
  In multiple-issue dynamically scheduled processor. 
  \begin{outline}
    \1 Schedule insns from multiple thds
    \1 Insn from independent thds execute when function units are available
    \1 Within thds, dependencies handler by scheduling and reg nenaming
  \end{outline}

  Example: Intel P4 HT. Two thds, duplicated registers, shared function units and
  caches.

  \subsubsection{Example MT}
  \textbf{CMT}: Switches between threads on each instruction causing the exec to be interleaved.
  Usually tone in a round-robin fashion. Pro: can hide short and long stalls. Con: Slows
  down exec of individual thds, since a thd rdy to exec wo stalls will be delayed by insns 
  from other thds.

  \textbf{FMT}: Switches thds only on costly stalls. Pro: Relieves need to have very fast 
  thd switching. Con: Hard to overcome throughput losses from shorter stals, due to pipeline
  startup costs

  \cgraphic{0.8}{img/mtex.png}

  \subsubsection{SMT Design Challenges}
  SMT makes sense only with fine-grained implementation. Larger resgister file
  needed to hold multiple contexts. Ensure that cache and tLB conflicts generated
  by SMT do not degrade performance.

  \subsubsection{Future of Multithreading}
  Multiple simple cores might share resources more effectively.

  \subsection{Vector Processing}
  An alternative classification:
  \cgraphic{0.8}{img/vecclass.png}

  \subsection{SIMD Processing}
  Single instruction operates on multiple data elements, in time or in space. 
  \begin{outline}
    \1 Array processor: Insn operates on multiple data elements at the same time using
    different spaces
    \1 Vector processor: Insn operates on multiple data elements in consecutive time steps
    using the same space
  \end{outline}
  \cgraphic{0.8}{img/arrayvsvector.png}

  \subsubsection{VLIW: vector HW without vector ISA}
  Multiple independent operations packed together by the compiler.
  \cgraphic{0.8}{img/vliw.png}
  Problem: Register file needs $3n$ ports, 2 read and 1 write per issue slot.

  \subsubsection{Vector Processors}
  A vector is a 1D array of numbers. Many scientific/commervial programs use vectors.
  A vector processor is one whose insns operate on vectors rather than scalar values.

  Basic requirements
  \begin{outline}
    \1 load/store vectors $\to$ vector registers
    \1 Operate on vectors of different lengths $\to$ vector len reg \texttt{VLEN}
    \1 Elements of a vector might be stored apart from each other in memory $\to$ 
    vector stride register \texttt{VSTR}
  \end{outline}
  Stride: distance in memory between two elements of a vector.

  A vector insn performs an operation on each element in sonecutive cycles. Vectir functional units
  are pipelined, each pipeline stage operates on a different data element.

  Vector instructions allow deeper pipelines
  \begin{outline}
    \1 Non intra-vector dependencies
    \1 No control flow within a vector
    \1 Known stride allows easy address calculation for all vector elements
  \end{outline}

  \subsubsection{Vector Processors Advantages}
  No dependencies within a vector, each insn generates a lot of work, highly regular
  memory access pattern, no need to explicitly code loops.

  \subsubsection{Vector Processors Disadvantages}
  Works (only) if parallelism is regular (data/SIMD parallelism). 


  \subsubsection{Cray-1}
  The first vector processor.
  \cgraphic{0.8}{img/cray1.png}

  \subsubsection{Vector programming model}
  \cgraphic{0.8}{img/vpm.png}
  
  Vector instrucion set advantages
  \begin{outline}
    \1 Compact
    \1 Expressive, tells HW that theses $N$ operations are
      \2 Independent
      \2 Use the same functional unit
      \2 ...
    \1 Scalable
  \end{outline}

  \cgraphic{0.8}{img/vectorcodeexample.png}

  \subsubsection{Vector Instruction Execution}
  By one heavily pipelined ALU or multple.
  \cgraphic{0.8}{img/vecpara.png}
  \cgraphic{0.8}{img/veclane.png}

  \subsubsection{Vector Chaining}
  Vector version of register bypassing. Introduced with Cray-1. With chaining, can 
  start dependent insn as soon as first result appears.
  \cgraphic{0.8}{img/vchain.png}

  \subsubsection{Vector Startup}
  Two components of vector startup penalty. Functional unit latency (time through pipeline),
   dead time or recovery time (time before another vector insn can start down pipeline).
  \cgraphic{0.8}{img/vstartup.png}
  \cgraphic{0.4}{img/vdt1.png}
  \cgraphic{0.4}{img/vdt2.png}

  \subsubsection{Vector Stripmining}
  If large vectors don't fit in \texttt{VLEN}, do $N$ operations with 64 elements
  and the remainder with $x$ elements.

  \subsubsection{Automatic Code Vectorization}
  Vectorization is a massive compile-time reordering of operation sequencing and
  requires extensive loop dependence analysis.

  \subsubsection{Memory operations}
  Load/Store operations move groups of data between registers and memory. Three types
  of addressing:
  \begin{outline}
    \1 Unit stride
      \2 congiuous block of information in mem
      \2 Fastest: always possible to optimize
    \1 Non-unit (constant) stride
      \2 Harder to optimize mem system for all possible strides
      \2 Prime number of data banks makes it easier to support different
          strides at full BW
    \1 Indexed (gather-scatter)
      \2 Vector equivalent of register indirect
      \2 Good for sparse arrays of data
      \2 Increases number of programs that vectorize
  \end{outline}

  \subsubsection{Vector Scatter/Gather}
  Want to vectorize loops with indirect accesses:
  \begin{lstlisting}[language=C]
for (i=0; i<N; i++)
  a[i] = b[i] + c[d[i]]\end{lstlisting}

  \begin{lstlisting}[language={[x86masm]Assembler}]
ld vD, rD # ld indices in D vect
lvi vC, rC, vD # ld indirect from rC base
lv vB, rB #ld B vect
ADDV.D vA, vB, vC # do add
sv vA, rA # store result\end{lstlisting}

  \subsubsection{Vector Conditional Execution}
  Want ro vectorize loops with conditional code:
  \begin{lstlisting}[language=C]
for (i=0; i<N; i++)
  if (a[i] > 0) 
    a[i] = b[i];\end{lstlisting}

  Solution: Add vector mask (or flag) registers and maskable vector instructions.
  \cgraphic{0.8}{img/vcond.png}

  \subsubsection{Vector Reductions}
  Operations that reduce a vector to a scalar (e.g. sum of all elements).
  \begin{lstlisting}[language=C]
sum = 0;
for (i=0; i<N; i++)
  sum += a[i];\end{lstlisting}

  Solution: Re-associate operations if possible, use binary tree to perform reduction.
  \begin{lstlisting}[language=C]
sum[0:VL-1] = 0;
for (i=0; i<N; i+=VL)
  sum[0:VL-1] += a[i:i+VL-1];
// now have VL partial sums in 1 vec reg
do {
  VL = VL/2;
  sum[0:VL-1] += sum[VL:2*VL-1];
} while (VL>1);\end{lstlisting}

  \subsubsection{Novel Matric Multiply Solution}
  Consider a matrix multiplication: 3 loops. Do need to do a bunch of reductions?
  No. Calculate multiple indep. sums within one vector register. Vectorize
  the second loop to perform 32 dot-products at the same time.
  \cgraphic{0.8}{img/vmtx.png}
  \begin{lstlisting}[language=C]
// multiply a[m][k] * b[k][n] to get c[m][n]
for (i=1; i<m; i++) {
  for (j=1; j<n; j+=32) {
    sum[0:31] = 0; // init vector
    for (t=1; t<k; t++) {
      a_scal = a[i][t]; // get scalar
      b_vec[0:31] = b[t][j:j+31]; // get vec
      // do vect-scal mul
      prod[0:31] = b_vec[0:31]*a_scal;
      // Vector-vector add into res
      sum[0:31] += prod[0:31];
    }
    // unit stride store of vec of res
    c[i][j:j+31] = sum[0:31];
  }
}\end{lstlisting}

  \subsection{Vector Processing in Modern ISAs}
  \subsubsection{The rise of SIMD}
  SIMD is good for applying identical computations across many data elements. It is
  energy efficient and tend to be bandwidth-efficient and latency-tolerant.

  \subsubsection{Multimedia (SIMD) Extensions}
  MMX follows a packed-SIMD execution model. A la array processing, yet much more limited.
  Other examples include ARM NEON, Intel MMX/SSE/AVX, RISC-V P (DSP) Extension.
  \begin{outline}
    \1 Single insn acs on multiple pieces of data at once
    \1 Common application: graphics
    \1 Perform short arithmetic operations (also packed arithmetic)
  \end{outline}
  \cgraphic{0.6}{img/simdadd.png}

  \textbf{Packed-SIMD is no Vector-SIMD}. Limited instruction set, limited
  vector register length, vector length is set in stone. Very short vectors, e.g., ARM NEON 
  has a VLEN of 128 bits (Cray-1 in 1976 had 1024 bits). VLEN is encoded
  in the instruction itself. Widest extension to date: \texttt{Intel AVX-512}.

  \subsubsection{Renaissance of Vector Processing}
  Cray-like vector processing is currently expierencing a renaissance during the last 
  few years. Main ISAs now include a vector processing extension: ARM SVE (scalabe vector
  extension) or RISC-V vector extension.

  \subsubsection{ARM SVE}
  No preferred vector length: VL is a hardware choice, 128-2048b in 128b increments. 
  Vector length agnostic (VLA) programming model: Write once, compile once, vectorize more loops.

  \begin{outline}
    \1 Aarch64 (scalar)
      \2 Ten iterations over an 8-byte register
    \1 NEON (128-bit vector engine)
      \2 Fout it. over a 16-byte reg + two it of a drain loop over a 8-byte reg
    \1 SVE (VLA vector engine)
      \2 Three it over a 32-byte VLA register with an adjustable predicate
  \end{outline}

  \subsection{RISC-V Vector Extension}
  Being added as a standard extension to the RISC-V ISA. An updated form of Cray-style
  vectors for moden microporessors. Still a WIP.

  \begin{outline}
    \1 32 vector registers, \texttt{v0-v31}
    \1 Each vector reg has an associated type, including
      \2 Number of bits in each element
      \2 Representation (signed, unsigned, floating)
      \2 Shape (scalar, 1D)
    \1 Number of vec regs and their type are configured with special insns before
    using the vector unit
    \1 Vect insns are \textit{polymorphic}, operation depends on the opcode and on the
    vector register operand types
    \1 Vec len reg \texttt{v1} controls the num of elements executed by each insn
    \1 Instructions can be \textit{predicated} by a mask
  \end{outline}
  \cgraphic{0.8}{img/rvvus.png}

  \subsubsection{Vector configuration}
  Can be done writing a number of control regs. Requested regs are zeroed. \texttt{MAXVL}
  depends on the number of vector regs and type. It should be possible to writea assembly
  code without knowing \texttt{MAXVL}.
  
  \begin{lstlisting}[language={[x86masm]Assembler}]
setvl xdst, xsrc\end{lstlisting}
  \texttt{vl} is set to \texttt{min(MAXVL, xrc)} also copied to \texttt{xdst}.

  \cgraphic{1.0}{img/riscvecadd.png}

  Important property: Minimal changes in data types, operation and length are also
  only minor changes in assembler code.

  % ---------------------------------------------------------------------------
  \section{GP-GPUs}
  % ---------------------------------------------------------------------------


  % ---------------------------------------------------------------------------
  \section{Heterogeneous SoCs}
  % ---------------------------------------------------------------------------


  \cgraphic{1.0}{img/archoverview.png}

  \begin{empheq}[box=\eqbox]{equation*}
    \begin{gathered}
      U=RI
    \end{gathered}
  \end{empheq}

  \begin{outline}
    \1 
  \end{outline}

   \begin{tabularx}{\linewidth}{l c c c}
    \hline
    {} & Single & Dual & Quad \\ \hline
  \end{tabularx}



    
\end{multicols*}

\setcounter{secnumdepth}{2}
\end{document}
